[
  {
    "path": "posts/2024-06-26-ai-success-begins-at-intake/",
    "title": "AI Success Begins at Intake",
    "description": "Overview of the cornerstone of DSAA's success: the project intake form",
    "author": [
      {
        "name": "Otis Ding, Michael Page",
        "url": {}
      }
    ],
    "date": "2024-06-26",
    "categories": [],
    "contents": "\nTechnology is moving at a pace faster than what weâ€™ve ever seen\nbefore, and this has led to significant changes in the healthcare\nindustry as well. The integration of artificial intelligence (AI) is\nbecoming increasingly critical. One key to the successful implementation\nof AI solutions in clinical settings lies in the effective communication\nbetween clinicians and data scientists. To that end, the Data Science\nand Advanced Analytics team at Unity Health Toronto has developed a\nproject intake form that lays the foundation for its success. The form\nserves this purpose by allowing clinicians to directly communicate the\nproblems they encounter and suggest potential solutions. Hereâ€™s a closer\nlook at the purpose of this intake form, its benefits, and how it\nsupports our AI program.\nThe purpose of the\nDSAA project intake form\nThe DSAA project intake form is a structured document used to gather\ncomprehensive information about clinical problems that need addressing.\nIt is designed to capture the full scope of an issue, from problem\nidentification to proposed solutions, and includes detailed sections on\ncurrent processes, potential improvements, and metrics for evaluation.\nThis form is part of the first phase in our project intake process,\naimed at understanding the problem, exploring available data, and\nassessing project feasibility.\nBenefits to the intake form\nThe project intake form brings with it several benefits. The first\nand most important of which is an enhanced communication. The form\nbridges the communication gap between clinicians and data scientists by\nproviding a formal and standardized way to convey clinical problems and\npotential AI-driven solutions.\nThe form also allows for structured problem solving. By requiring\ndetailed descriptions of the problem, current conditions, and proposed\nsolutions, the form ensures that all aspects of an issue are considered.\nThis comprehensive approach facilitates thorough exploration and\nunderstanding of the problem.\nData is extremely important for solutions considering AI or machine\nlearning. Thankfully, the form helps to facilitate data-driven decision\nmaking. The form emphasizes the need for data and metrics, encouraging\nclinicians to quantify issues and outcomes. This focus on data ensures\nthat proposed solutions are based on solid evidence and measurable\ngoals.\nFinally, the form helps to create a collaborative approach to problem\nsolving. By involving various stakeholders into the process, the form\npromotes the collaboration between different professionals. This\nincludes input from clinical leads, subject matter experts, and other\nstaff members who will be impacted by the solution.\nHow the intake supports\nour AI program\nThe most important benefit this form has for our AI program lies in\nhow it allows us to identify pertinent problems faced by those in our\ncommunity. The form helps us identify and prioritize clinical problems\nthat are suitable for AI solutions. By clearly defining the problem and\nits impacts, we can focus our efforts on projects with the highest\npotential for improving patient care and operational efficiency.\nThe form also kicks off our feasibility assessment. It gives an\nappropriate starting point for evaluating the availability of data,\npotential barriers to implementation, and the expected impact on key\nmetrics. This rigorous assessment ensures that only viable projects\nproceed to the development phase. Finally, the detailed information\ngathered through the form guides the development of AI solutions.\nUnderstanding the current process and pain points allows us to design\nsolutions that are tailored to the specific needs of the clinicians and\nthe healthcare system.\nAll in all, the DSAA project intake form is a crucial tool in our AI\nprogram, enabling effective communication between clinicians and data\nscientists, ensuring structured approach to problem solving, and\nfacilitating the development of impactful AI solutions. By leveraging\nthis form, we can better understand clinical problems, assess the\nfeasibility of AI interventions, and ultimately improve patient care and\noperational efficiency in our healthcare system.\nThe intake form and\nprocess in a nutshell\nOur intake form is designed to be user-friendly, ensuring you can\nseamlessly provide all the essential details needed to kickstart a\nsuccessful project. Right from the start, it helps you pinpoint the\nproject team responsible for the initiative, making sure\neveryone knows whoâ€™s in charge.\nNext, it guides you through crafting a clear and concise problem\ndefinition, accompanied by an analysis of the current\nconditions. This step ensures that we fully understand the existing\nchallenges and processes in order to set a solid foundation for our\nproject.\nThe next step involves a vision of the future state,\ndetailing how AI will be integrated into specific workflows. Whatâ€™s also\nimportant is an emphasis on the evaluation metrics which tell\nus how to measure impact and if weâ€™ve successfully hit our goals. We\nalso make sure to call out any implementation risks of the\nproject before any part of the project begins.\nThis forward-thinking approach helps us align our goals and ensure\nthat our AI solutions are both practical and impactful.\nIn a future post, weâ€™ll make available a copy of our intake\ndocument.\n\n\n\n",
    "preview": {},
    "last_modified": "2024-07-16T15:36:13-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-05-21-wait-you-guys-are-writing-unit-tests/",
    "title": "Wait, you guys are writing unit tests?",
    "description": "A reminder. [5 min read]",
    "author": [
      {
        "name": "Chloe Pou-Prom",
        "url": {}
      }
    ],
    "date": "2024-05-21",
    "categories": [
      "post-miscellaneous"
    ],
    "contents": "\n\nContents\nWhat happened?\nThe Big\nUpdate\n\nThe epic highs and lows\nThe failing\ntest\nThe fix\n\nWhatâ€™s next?\nMore unit\ntests?\nContinuous integration?\n\n\n\nConfession: I rarely write unit tests. ðŸ˜… 1\nThat being said, I did recently run into an issue that was identified\nby a unit test!!\nWhat happened?\nThe situation: We recently updated Data Virtuality\n(DV), our data integration platform which consolidates data from various\nhospital source systems and lets us interact with data through a unified\nSQL API.2\nMany of our deployed applications rely on DV. Itâ€™s a vital\nproduction tool for our team.\nThe Big Update\nIt was time to update the DV version.\nBig updates are always scary so we had started\npreparing for this a while ago. We had a protocol in place. We\nwere headed into this upgrade, ready to roll back to an earlier version\nif ever anything went wrong.\nThe upgrade was happening after hours to limit the impact on\nend-users. I wasnâ€™t directly involved in the upgrade, but I was on\nstandby with my phone set to max volume in case I received a panicked\ncall in the middle of the night.\nThe Big Day arrived andâ€¦\nâ€¦the upgrade went well!\nApplications continued running as expected!\nExceptâ€¦\nThe day after, I was working on a project post-update. Before pushing\nmy changes to Gitlab, I ran the unit tests in my projectâ€¦\nAnd saw that one of the tests failed!!\nThe epic highs and lows\nThe failing test\ntest_that(\"bad sql to dv post api returns 400\", {\n  sql_stmt <- \"SELECT * FROM DoesNotExist LIMIT 1\"\n  dv_post_sql_api_fail_strategy <- dv_post_sql_api_connection_strategy_creator(sql_stmt)\n  dv_fail_query_response <- dv_details_binder(dv_post_sql_api_fail_strategy)\n  \n  response <- dv_fail_query_response()\n  status <- as.character(response$status_code)\n  expect_equal(status, '400')\n})\nBasically, the above unit test3 checks that sending a\nbad SQL query through the DV API should return a 400 status code.\nHowever, this test started failing because, after the update,\nincorrectly formatted SQL statements now return a 200 code\nstatus instead of a 400 code status!!!!!\n> response <- dv_fail_query_response()\n\n> httr::content(response)\n[[1]]\n[[1]]$errorType\n[1] \"SQLException\"\n\n[[1]]$errorMessage\n[1] \"Unexpected error when executing query without buffer cursorId=118818 sql=SELECT * FROM DoesNotExist LIMIT 1\"\n\n> response$status_code\n[1] 200\nAll of this was identified because of my failing unit test.\nThe fix\nFairly straightforward.\nWe updated the way in which we handle errors. Instead of just\nchecking a responseâ€™s status code, we also now check the response\nbody.\nWhatâ€™s next?\nMore unit tests?\nDefinitely.\nAs a team, weâ€™ve been including more unit tests in our projects.\nThese unit tests range from checking data processing, validating model\npredictions, and ensuring that our pipelines behave as expected. As we\nwrite more unit tests, weâ€™ve been asking ourselves the following\nquestions:\nWhen should we write unit tests?\nHow do we write unit tests on data?\nWhat does a unit test for a model look like?\nItâ€™s difficult to answer these questions4.\nContinuous integration?\nCurrently, I run my unit tests manually. I regularly\ndo this before pushing changes. Itâ€™s been working (and hey, in this\nparticular instance, it uncovered something serious!!), but itâ€™s not\nactually enforced. Weâ€™re using the honor code here5.\n\nA better approach would be to add continuous integration. In an ideal\nworld, changes to Gitlab would trigger the unit tests to start running\nautomatically.\nAnyway, this was a long-winded way to remind myself that yeah, unit\ntests help. ðŸ™‚\nWill I start writing more unit tests after this? \n\nYes, I know. Theyâ€™re important and they help find\ncritical failures. Just enter â€œAre unit tests worth it?â€ in your\nfavorite search engine, and youâ€™ll end up with a mountain â›°ï¸ of blog\nposts, StackOverflow replies, Reddit comments, heated discussions, etc.\nthat tell you: yes, unit tests are worth it.â†©ï¸Ž\nWe like Data\nVirtuality because:  â†’ It has a layer of abstraction between\nsource systems and data consumers. This means that, as a data scientist,\nI can access real-time data from one view. â†’ Any changes to source\nsystems are dealt with at the virtualization layer and donâ€™t affect the\nuser view. â†’ Query results can be reused in different places to\navoid pulling from the source system multiple times. â†’ Additionally,\nif one of the source system fails, data from the previous update are\ncached.â†©ï¸Ž\nAlright, technically itâ€™s more an integration\ntest, since weâ€™re testing an external dependency.â†©ï¸Ž\nPerhaps we can try answering these in a future blog post\nðŸ˜‰â†©ï¸Ž\nPre-commit/pre-push hooks could help here, but they also\nrely on the honor code.â†©ï¸Ž\n",
    "preview": "posts/2024-05-21-wait-you-guys-are-writing-unit-tests/unit_tests.png",
    "last_modified": "2024-07-16T15:11:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-12-22-dsaa-wrapped-2023/",
    "title": "DSAA Wrapped 2023",
    "description": "Reflecting on 2023. [6 min read]",
    "author": [
      {
        "name": "Meggie Debnath, Phyllis Tian, Rida Malik, and Michaelia Banning, on behalf of DSAA",
        "url": {}
      }
    ],
    "date": "2023-12-20",
    "categories": [
      "post-miscellaneous"
    ],
    "contents": "\nItâ€™s the end of the year, and that means itâ€™s time for another DSAA\nWrapped!\nOur DSAA Wrapped is based on Spotify Wrapped, an annual data-driven\nsummary of each userâ€™s listening habits and trends over the past year.\nThese recaps are showcased in engaging visual summary cards, which are\nshared widely with friends and across social media platforms.\nHereâ€™s our version, showcasing some of our project deployments and\ntop team moments. This year, we focused heavily on evaluating some of\nour existing tools that have been deployed, and on projects that aim to\nimprove patient flow!\n\n\n\nProjects\nThis year, we deployed a whopping 14 projects across\nall of our product families! Thatâ€™s more projects than the number of\npizzas weâ€™ve consumed this year (okay, maybe not that many, but\nclose).\nOur product and data wizards worked their magic, and the project that\nstole the show in terms of views was once again none other thanâ€¦ the\nOperations Centre! The Ops Centre project offers an\noverview of patient movement within the hospital. It comprises various\npages that display real-time information related to planned admissions,\noccupancy, upcoming discharges, bed cleaning, portering, and other\ncritical elements necessary for a holistic understanding of patient\nflow.\nThe Avengers of data science\nWhen it comes to assembling a team, our project with the most team\nmembers, Predicting Clinical Readiness with 14\nmembers, resembled an Avengers movieâ€”full of diverse talents,\noccasional challenges, and ultimately, a blockbuster success. The aim of\nthis project is to identify General Internal Medicine patients at\nSt.Â Michaelâ€™s Hospital who are likely to become medically stable so that\ndischarge related activities can start earlier in the patientâ€™s hospital\nstay, ultimately achieving better patient flow and minimizing\ndelays.\nProduct family royalty ðŸ‘‘\nIn a year that saw projects popping up like mushrooms after rain, the\nPatient Flow family stood tall, boasting the crown for\nthe most projects.\nSpeaking of product families, we had a new product family join us\nthis yearâ€¦ the Signal1 family, a collaboration between\nDSAA and Signal1, bringing our total\nnumber of product families to 6! They are also the\nfamily with the most team members at 11 members.\nSprint improvement weeks ðŸ› ï¸\nIn 2023, we had 2 sprint â€œimprovementâ€ weeks which\nwere as fun and productive as ever, as we reached a milestone of\n10 sprint weeks completed till date! From cross-team\nbrainstorming sessions to infrastructure improvements, we emerged\nstronger, wiser, and with a new found appreciation for the power of\nespresso breaks. In our 10th sprint week, we had 32\ntasks in the backlog, 20 accomplishments\ndemoed, and 4 pizzas eaten.\nThe learning never stops\nAt DSAA, the learning never stops!\nIn our team-wide journal clubs, we learned about:\nThe Five\nIdeals\nLawsuits\nin the domain of generative AI\nProblems\nin the deployment of ML models in healthcare\nClean code in\nPython\nand more!\nThis year, our Advanced Analytics (AA) sub-team had more than\n10 journal club sessions that dived into the worlds of Natural\nLanguage Processing (NLP), performance optimization, and project\nevaluation. The AA team also had the chance to take their learning\noutside of the office, by going on a field trip to the Thomas Fisher\nRare Book Library at U of T, which had an exhibit of Data\nVisualization Throughout History.\nWe were also fortunate to have Posit host a number of workshops for\nus, covering the topics of pointblank and MLOps\nwith vetiver.\nSharing ideas\nThis year, we didnâ€™t just learn new things, our team members also had\nopportunities to exchange ideas, and share insights and experiences\nthrough talks and conferences. Our team presented about the Hemodialysis\nproject at the Renal\nInsight User Group Meeting, shared lessons from implementing AI in\nhealthcare from the CHARTwatch project and beyond, and discussed estimating\nROI for analytics and data projects.\nWe also hope to continue to raise our count of published blog posts!\nThis past year, weâ€™ve had team members share 3 blog\nposts about developing\nand deploying AI solutions in the hospital, creating\nreproducible environments for building medical imaging machine learning\nmodels with Nix, and discrete\nevent simulation in healthcare.\nThe Party Planning Pandas\nand beyond! ðŸŽ‰\nLetâ€™s recount some of our memorable team moments from this past year,\norganized by our social committee: The Party Planning Pandas ðŸ¼.\nNerf Gun Challenge: With our in-office stash of nerf guns, during\nthis challenge, our team learned that precision and luck go hand in\nhand.\nValentineâ€™s Day & Zoom trivia: This year, our data science team\nembraced AI art generators to craft charming (and maybe a little scary)\nValentineâ€™s Day cards as well as adorable baby animal versions of our\nteam.\n\n\n\nSummer Picnic: Sun, fun, and ICE CREAM â€” our summer picnic at\nRiverdale Park in Toronto included some painting with marshmallows, an\nice cream truck, and a scavenger hunt that turned into an extended visit\nto Riverdale Farm!\nHalloween at DSAA: Our team members transformed into a raccoon, Snow\nWhite, Wednesday Addams, and everything in between.\nAxe Throwing: We traded our laptops for axes and unleashed our inner\nwarriors. Letâ€™s just say, our aim in data science translated\nsurprisingly well to hitting bullseyes.\nAs we wrap up this yearâ€™s Wrapped, letâ€™s share a round of applause\nfor our incredible team at DSAA and all of our collaborators. Hereâ€™s to\na year of innovation, collaboration, pizza, and data-driven\nsuccesses!\nWeâ€™ll leave you with some of our AI-generated holiday cards, happy\nholidays!\n\n\n\n\n\n\n",
    "preview": "posts/2023-12-22-dsaa-wrapped-2023/dsaa_wrapped_preview.PNG",
    "last_modified": "2024-04-25T17:54:21-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-17-introduction-to-discrete-event-simulations-in-healthcare/",
    "title": "Introduction to Discrete Event Simulations in Healthcare",
    "description": "An introduction to Discrete Events Simulations (DES) in healthcare with an overview of the use of DES for modelling outpatient services appointments at Providence Healthcare.",
    "author": [
      {
        "name": "Kevin Wang, Neil Mistry",
        "url": "https://www.linkedin.com/in/mingkun-wang/, https://www.linkedin.com/in/neilmistry/"
      }
    ],
    "date": "2023-04-18",
    "categories": [],
    "contents": "\n\nContents\nDiscrete Events Simulation at Providence Outpatient Care & Services Covid-19 Recovery scheduling\nBackground\nAn Example of DES in the Real World: Providence Outpatient DES Project\nHow we approached a DES solution\nConclusions\nLimitation\nDES software\n\n\nDiscrete Events Simulation at Providence Outpatient Care & Services Covid-19 Recovery scheduling\nDiscrete Events Simulations (DES) are a method of modeling the operation of real world systems. DES does this by modeling a series of events in time. Events are ordered by time of occurrence, and the DES system updates its state as it traverses through each event.\nThis type of simulation is useful at modeling a system with many uncertain (or probabalistic) steps. For example, in the Emergency Department, you wonâ€™t know when your next patient will arrive and you wonâ€™t know the condition this patient will arrive in. In systems where there are a complex number of steps, many different resources, inputs and outputs, DES may be one of the better ways to approach modelling.\nDES are built primarily for two purposes, to assess: (1) the current system and uncover bottlenecks, and (2) how changes to our system will impact its performance. We typically do this to test potential improvements to our system. This could be a change in our process, additional resources, changes to inputs or outputs and changes in our schedules. DES will allow us to measure things that are important to us in the current state and with changes to our system.\nThe underlying statistical theorems that support DES come from Queueing theory. A classic example of queueing theory is the Call Center. In a Call Center you have incoming calls, and you know roughly how many calls you can expect in any given time period, such as a Monday, or between 5-8pm on a Friday. Therefore you know the frequency of calls and have some expectation on the variation of calls throughout the week.\nPoisson Call Center QueueHowever you do not know exactly when your next call will arrive (remember this is probabilistic or uncertain). This is known as a Poisson arrival process, and is modeled as:\nPoisson Call Center QueueLambda \\(\\Lambda\\) is the only parameter that Poisson depends on. This is your arrival rate (on average). Queueing theory works well for a simple queue like this, where you have a single entry point for your customers, a single type of service (answering the call) with some probabalistic length of time and one exit for your customer (end of call).\nHowever with more complex system, like those in hospitals and other healthcare settings, we may have patients who arrive in multiple ways, have multiple possible treatments or interventions, and could have multiple exits (admission, discharge), the formulas for Queueing theory become impractical to model these systems. That is when we need DES!\nBackground\nSimulation had long been used in manufacturing as there can be considerable variability in process outputs in a manufacturing line. In addition to variability, having a birdâ€™s eye view of the entire process can allow us to easily assess bottlenecks in our system. Bottlenecks could be caused by variability in our process outputs, overproduction, excess inventory, high down time or failures in machines, or a lack of staff or resources.\nWe can then make assessments to changes in our system. Letâ€™s say for example we add an additional staff member at a point in our process. Does this improve our bottleneck? What if we replicate a few steps in our process and run parallel production? Will the cost to implement this strategy be outweighed by the benefits gained by a reduced bottleneck in this area? We can assess this in our simulation before trying it in our real world system.\nIn healthcare we see DES used, for examples, to evaluate and assess operational changes Emergency Departments, Operating Room Suites, Intensive Care Units, and Inpatient and Outpatient units. At Unity Health Toronto we used DES in the past to help us evaluate changes to the physical layout in the Emergency Department (ED), assess changes to physician scheduling in the ED and most recently, assess patient appointment scheduling for outpatient services.\nAn Example of DES in the Real World: Providence Outpatient DES Project\nProvidence Healthcare (one of the three hospitals in the Unity Health Toronto network) had challenges scheduling outpatient appointments with the onset of the second wave of the Covid-19 in 2021.\nOutpatient Services consists of multiple clinics providing medical appointments. Each patient arrives, registers, and wait for the appointment in the waiting areas or vehicle. They will then be escorted to clinical waiting areas or directed into a room. After the appointment, they may continue to wait in the clinical waiting areas for follow-ups or leave the hospital. During this process, there may be congestion resulting from a combination of patient behaviors, system resource and capacity (early-arrival of wheel-trans patients, max room capacity, lack of therapists); A smooth outpatient process is important for the hospital system to run efficiently for optimal patient care and quality of service.\nWe modeled the clinic in its current state and explored alternative appointment schedules to increase the number of patients flowing through outpatient services while maintaining social distancing practices.\nOutpatient clinic appointment process at Providence Healthcare in August 2020Under certain pandemic-based restrictions, all individuals were required to maintain social distancing between each other. To minimize unnecessary contact, the Outpatient Service Department wanted to control the maximum number of patients in each area of the department. Planning to re-open more outpatient services (and to reduce the backlog created by shutdowns during the first two waves of the pandemic), the Outpatient Service Department modeled new schedules by adding additional appointments with a combination of more staffing, longer hours, more patients and additional working days.\nHow we approached a DES solution\nThe solution that was developed for the Providence Healthcare Outpatient Service Department consists of two parts:\nA simulation model that recreates the physical space capacity and analyzes the system & process performance\nDesigning a web user interface that requires users to input key features of the simulation for testing different scenarios. In addition, the model would output key performance indicators to help access the performance of the proposed plans/system.\nThe primary input that our user can manipulate to test out scenarios is the appointment schedule. This appointment schedule includes a breakdown of the types and quantities of appointments that would be seen in Outpatient Services by the day of the week (Mon, Tue, Wed, and so on).\nThe simulation model consists of three main parts:\nA defined object class called â€œPatientâ€ with all necessary attributes and pre-defined calculation functions for results and outputs. Attributes include but not limit to: appointment type, appointment clinic, and probability of the patient needing wheel chair transportation from their home.\nThe second part consists of logging, status update, capacity checking and calculation functions for the simulation process. Detailed information of patient arrivals are calculated and recorded through these supporting functions.\nThe last part is the simulation model function, where it reads in user defined clinicsâ€™ capacity, registration time, and an appointment schedule from an inputted excel file. The model then reads the patient schedules and generates patients based on their scheduled appointment time.\nAll of our performance metrics (wait time, current number in system) are recorded through a trial of runs and outputted to the user. Utilization plots for clinics and service providers are also plotted.\nConclusions\nWe can use DES to test how changes to our system can affect the outcomes we are interested in improving.\nAt Providence health, we were able to test alternative patient schedules to measure their impact on throughput on our outpatient clinics. We tested several schedules that would increase the number of patients we would see, and our DES could tell us if we would maintain our social distancing requirements. Ultimately we were able to create schedules that could increase the number of patients we could see, even with our current staffing schedules.\nLimitation\nOne limitation in our DES is the service time for each appointment. We simplified the length of time a patient may spend at an appointment by ignoring some of the variabilities in process times due to the lack of data. Instead of fitting a time distribution, we had to use a more deterministic approach by assuming a static service time for all the servicing times involved in the model.\nDES software\nThere are many available software applications that can build DES models. Some provide an installer and can be used on a windows PC (ARENA, Anylogic, FlexSim, Simul8, etc). This is great for use in 1 time simulations when you will infrequently reassess your system.\nWe also have DES software that can be hosted remotely and can accept simulations as jobs (Simul8 has this feature). This type of software can be useful when you want to run a simulation more frequently, perhaps even live to predict possible incoming bottlenecks.\nThere are also many open source DES packages now available. Examples include DESMO-J, which is available in Java, Facsimile, in Scala, and SimPy, available in Python.\nData Science and Advanced Analytics wanted to host an application for users to enter patient schedules and run the simulation remotely and on-demand. We chose SimPy to accomplish this.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-02-28-medical-imaging-with-nix/",
    "title": "Reproducible medical imaging software environments in Nix, or, Living in the future is hard.",
    "description": "My journey creating a reproducible environment for building medical imaging machine learning models with Nix.",
    "author": [
      {
        "name": "Chris Hammill",
        "url": {}
      }
    ],
    "date": "2023-02-28",
    "categories": [
      "infrastructure",
      "deployment-environments",
      "nix"
    ],
    "contents": "\nIâ€™ve been thinking for a long time about how to build software environments that are resilient to the ravages of time and that others can pick up and use effectively at their leisure. So when in the course of my work at DSAA I needed to develop an effective environment for doing machine learning on medical imaging data, I jumped at the chance. The fruit of that labour is:\nhttps://github.com/LKS-CHART/medical-imaging-nix\na working environment of software, defined in software. It allows me or anyone else to reproduce my work or use the exact software computing environment I use. What do I mean by environment? In this case I mean isolated collections of programs, packages, and libraries that can be used together, but donâ€™t interact with the rest of your software. The concept might be familiar if youâ€™ve worked with renv, conda, or lmod.\nThe project is powered by nix, a futuristic technology gaining a foothold among certain groups of software developers devops people, and programming language researchers1. Renowned for its ability to almost fully deterministically and reproducibly build software, it enables strongly isolated environments.\nWhat is medical-imaging-nix?\nThe repo above is a nix project2 that generates a software environment that can be used as a starting point for future projects.\nThe environment contains R, python, jupyter, a suite of tools for working with medical imaging files, pytorch and tensorflow with GPU support enabled. All3 the things a medical imaging data scientist needs to hit the ground running on a new project. It can be forked and expanded to include other kinds of dependencies, you could add julia4, rust, fortran, and many others with just a few lines of code.\nThe environment can be compiled into a apptainer container5 for portability to places that donâ€™t have nix. To get started with the environment you need nix installed with flakes enabled but a new project can be initialized as simply as:\nmkdir my-awesome-project\ncd my-awesome-project\nnix flake init -t github:LKS-CHART/medical-imaging-nix\nnix develop\nthe first time you run this it will take a very long time, because it will build most of the software universe from scratch for you6. But after that all subsequent calls to nix develop will quickly drop you into your shiny new environment. If youâ€™re ready to package up your environment into a container you can run:\nnix build\nthis also takes a while, so start it before a meeting. But once thatâ€™s done you can send your environment image to anywhere it might be needed.\nHow does it work\nI decided to build the environment using nix7, it is a package manager, a build system, and a programming language. The programming language allows you to write code that builds software, the build system builds the software and caches it in a content addressable store, and the package manager-like features give you access to those packages. This allows you to generate supercharged versions of conda environments, renvs, and can even obviate the need to use docker. Iâ€™ll explain the advantages in a moment.\n\nyou, a savvy expert: but Chris, what about conda, renv, groundhog, pacman, rstudio package manager, and docker?\n\n\nme, an esoteric technology astronaut: what about â€™em?\n\nThe nix ecosystem also provides a curated set of a packages referred to as nixpkgs, akin to conda, or debian packages available from apt. Nixpkgs is the largest set of packages provided by any package manager, so on paper this should mean that building environments with nix should be easier than any of the alternatives, you can just grab your packages from the massive set of packages in nixpkgs.\nBut whatâ€™s different?\nIf you talk to users of nix and ask them why nix, the argument is rarely centered on the size of nixpkgs although that is certainly a plus. Youâ€™re more likely to hear about how the builds are isolated, they donâ€™t interact with the rest of your software, that the builds are specified in a single programming language, and that versioning of the entire repository can be done through git. These are no mean feats.\nBuilding software is really hard. If youâ€™ve ever wanted to build someoneâ€™s project from source, especially complicated modern projects, it can be a trying ordeal. Software can be built with make, cmake, ymake, autotools, setuptools, R CMD INSTALL, ninja, bazel, shake, and many many more build systems and tools. The extremely brilliant devs who designed the nix build system found ways to hook into most of the other common build systems, and then ecosystem contributors used those tools to create reproducible recipes to build each of the 81k+ packages in nixpkgs. Adding your own additional packages using the nix language and build system is relatively simple8, so as your project grows it can absorb new packages into your declaratively specified software environment9\nOnce youâ€™ve built the software, you then have to worry about dynamic dependencies. Dynamically linked C programs rely on dependencies that are hanging around on disk. A program called the linker says go find this particular feature you use when you run the program, if the dependencies providing those features have changed on disk your results could be different. Dynamically linking C programs/libraries relies on a byzantine collection of different types of mutable global state. There is your system default library locations (global state, disk-backed), LIBRARY_PATH/LD_LIBRARY_PATH (global state, shell environment local and disk-backed), pkg-config is sometimes used to configure linking (disk backed global-state pointing to disk-backed global state). Global state has the potential to do like state and changeâ€¦ state. This means your results are balanced on a house of cards where shifting one part of the state could break your results.\nDonâ€™t believe me that this is a real issue? What about silently getting the wrong results for your numerical code because you had LD_LIBRARY_PATH or update-alternatives set incorrectly, oof.\nOr not being able to build your software at all because one of your package managers got in the way. This may sound like puritanical nerd worries, but these are real issues Iâ€™ve experienced in practice. I was soured on conda long ago when even having it on my executable path broke my ability to build the R package I was developing for work. What happened? Condaâ€™s addition to my PATH overrode my system h5cc a compiler wrapper for building C libraries that depend on HDF5, this linked in the wrong version of HDF5 and prevented my R package from building. I had not asked conda for h5cc, it was pulled in as a dependency of some arbitrary conda package I was using. Itâ€™s wasnâ€™t just condaâ€™s fault, the R package itself was an eldritch horror of an autotools build, ditching conda was easier than fixing the R build to ignore condaâ€™s h5cc.\nNix gets around these issues by not using the system default libraries wherever possible, avoiding LIBRARY_PATH/LD_LIBRARY_PATH wherever possible10, and making the places where state is unavoidable immutable (your nix store of built things is read-only). So builds are hermetic and isolated, you can happily have multiple versions of the same C library running around without paying any extra attention to where your C dependencies are coming from. This means you donâ€™t need a separate docker container to have an alternate universe of C libraries to make sure your analysis works, you just have it beside all your normal stuff, and thatâ€™s relevatory when youâ€™ve been bitten by these problems enough times.\nThese builds, instead of being an imperative sequence of commands to run to build and install software to a specific place, are written in the nix programming language, a functional programming language designed to make it easy to modify and tweak the build and dependencies so that your software environment is fully specified in code.\nBy contrast conda has 8000 prebuilt packages, ubuntu offers an admirable 36k. These builds assume something about the directory layout of your system, and they can be broken by updating system packages using other package manager.\nNix also empowers you to be your own package repository, significant effort has gone into making builds fully deterministic where possible. This means once Iâ€™ve built â€œpySweetDataToolR.jlâ€ I can give it to you, if weâ€™re on the same architecture you can just slot the relevant parts of my nix store into yours, so for a medium or larger organization you can set up a global cache of nix builds on a server that can be downloaded by each user. For smaller orgs you might be able to get away with a single nix-store that everyone can share. No more N numpys per employee.\nWhereâ€™s the rub?\nSo far I might have sounded effusive, if not fanboyish about the space alien wizard technology that can replace apt, conda, renv, etc. but there are real and significant sharp corners to nix and nixpkgs especially for data science. First off while nixpkgs includes every single package available on CRAN at the time of last snapshot, its coverage of pypi is piddly. Nixpkgs includes ~5200 python3.10 packages, whereas pypi has ~432k packages11. In order to put together the data science environment I built, I needed to package or modify 33 python packages. Some medical imaging related, some for working with jupyter notebooks, some machine learning related. And while generally not very challenging once I got the hang of it, some are quite thorny to package. Most are properly built from source, but some are just a thin wrapper around the wheels available on pypi, which defeats the purpose of nix12. Nix also encourages you to run the full test suite for the packages, but often test code is scrubbed from packages on pypi, so unless you get the code from github you may not have the tests. And you might need to disable some tests because nixâ€™s test environment is immutable, and builds are disallowed from downloading supplemental data by default, which sometimes breaks test code, so either you have to patch the tests yourself or you turn them off (so guess which you choose if youâ€™re pushing for a deadline).\nSo life in the future is tough, because you become part of the team building it. The future I mean. Since starting my nix journey several years ago Iâ€™ve contributed code to nixpkgs more than a few times, but when you are under pressure to achieve actual business goals it can be very frustrating to have to solve these problems yourself. Whatâ€™s worse is I still feel like a beginner with nix despite having years of experience. Others Iâ€™ve encountered like the documentation, participate in the discourse or IRC channels and feel comfortable using nix, my experience hasnâ€™t been so pleasant, with most issues Iâ€™ve encountered feeling almost ungoogleable, different learning styles I guess?\nThe other place Iâ€™ve found nixpkgs to be frustrating to use for python, is upgrading. Unless youâ€™re maintaining your own branch of nixpkgs you are somewhat at the whims of other nixpkg contributors as to what gets upgraded when. And due to a potentially poor choice on strategies the most up-to-date version of nixpkgs may contain a large set of temporarily broken python packages. Often upgrading one package will break many packages that depend on it, not only because the code becomes incompatible, but because it is fashionable in python packaging to set strict version upper bounds, so the package wonâ€™t even build (so we canâ€™t check if all the tests still pass with the new version). So I find myself in the position of checking out old versions of files from nixpkgs to build my package overlays when I need to downdate a certain dependency. This is tedious and I should probably switch to maintaining my own version of nixpkgs with my downdates and modifications, but this makes it harder to share with others.\nSo this points to the biggest advantage of conda over nix, when conda does not have a package it gets it from pypi, nixpkgs cannot fail over to grabbing from pypi and installing with pip, it also canâ€™t do dependency solving, if the nixpkgs version of a python lib isnâ€™t compatible with another, you have to go find a satisfactory version yourself (or cheat and lie about the version requirements^ [I was reminded after the first draft of this post about the existence of pythonRelaxDeps, a way to automatically soften version requirements, I may switch to use this in the future.]). This is painful. There are two nix projects that have aimed to address this problem, mach-nix which has been abandoned13, and poetry2nix which may solve some of my woes but I havenâ€™t tried it yet.\nAnother advantage alluded to already is speed. Compiling things takes time, so unless youâ€™re always getting prebuilt binaries from the nix servers you can be in for long build times. Nix is good at not duplicating build work, but sometimes rebuilding is unavoidable. Say you want to use a newer version of cuda, or gcc, most of your environment will need to be rebuilt14\nFinally, the last difficulty is with irreducible system dependencies. Nixpkgs, when youâ€™re not using the nix operating system, does not interact well with graphics drivers, there is a wrapper project I use called NixGL which gives you access to graphics drivers and allows you to run programs that use CUDA15. However this has meant I needed to hardcode my graphics driver into the nix flake, severely hindering portability16.\nIs this worth it?\nI donâ€™t know. Iâ€™ve sunk considerable cost into building this environment so one might reasonably expect me to be quite biased.\nWould cutting down my effort by 75% at the risk of some amount of computational irreproducibility and unportability be worth it?\nIs this a case of good enough practices trumping best practices?\nIâ€™m not sure, but now that itâ€™s built I know that I will be able to revisit this exact environment for many years to come. Iâ€™ve acquired the skills to fix package sets as issues arise, so itâ€™s not a large burden any longer to develop. If I can iron out a few more details about merging python package sets from multiple versions of nixpkgs I would think that this is substantively superior to managing my software with conda, apt, renv, and docker.\nWould I encourage others to adopt the strategy of creating a nix environment from scratch for each new project? Probably not, at least donâ€™t go it alone. Sharing working flakes and overlays for data scientists to springboard off of makes this type of reproducible, portable, futuristic software versioning possible17, but keep your novelty budget in mind, you might be wise to pick boring technology.\nFuture directions\nMy plans are to continue improving and refining this environment. Future directions Iâ€™d plan to investigate are:\nImproving the code quality of the overlay, for example converting from explicit find-replaces to pythonRelaxDeps in the code that generates python package builds.\nContributing upstream. Iâ€™ve had a few PRs back to nixpkgs go through, and some stagnated from slow feedback cycles. I should commit to giving back to the community when capacity allows.\nFigure out how to interleave package version sets for python. In general if I want a version of software from nixpkgs commit A and some other software from commit B, I can just use two versions of nixpkgs in my flake. Unfortunately it is not so simple to do with python packages, instead of building the desired package against your versions of all other python packages it will try to bring in its own, which breaks python installs. I suspect I can â€œborrowâ€ ideas from mach-nix for this.\nGeneralize the graphics driver pinning, or at least provide a shell script to automatically patch the flake to the current system graphics driver.\nContinue adding/packaging useful software so that this environment stays productive and portable for years to come.\nConveniently for me, one of the very few other nix medical imaging machine learning people just joined our team, so Iâ€™m sure heâ€™ll have many useful suggestions to improve things.\nI like many others think that approaches like nix will become more prevalent in the future, and itâ€™s nice to get a taste of it now even if there are pain points.\nAcknowledgements\nThanks to Ben Darwin, Chloe Pou-Prom, Meggie Debnath, Dimuth Kurukulaarachchi, and Derek Beaton for providing useful feedback on drafts of this post.\n\nComments\nYou can use your Mastodon account to reply to this post.\nReply\nReply to cfhammill's post\n   With an account on the Fediverse or Mastodon, you can respond to this post. Since Mastodon is decentralized, you can use your existing account hosted by another Mastodon server or compatible platform if you don't have an account on this one. \n  Copy and paste this URL into the search field of your favourite Fediverse app or the web interface of your Mastodon server.\n  \n    Copy\n    Close\n  \n\nLoad comments\nYou need JavaScript to view the comments.\n\n  .mastodon-comment {\n  background-color: #00000007\n  }\n\n    const dialog = document.querySelector('dialog');\n    document.getElementById('replyButton').addEventListener('click', () => {\n        dialog.showModal();\n    });\n    document.getElementById('copyButton').addEventListener('click', () => {\n        navigator.clipboard.writeText('https://mastodon.social/@cfhammill/109944483497018229');\n    });\n    document.getElementById('cancelButton').addEventListener('click', () => {\n        dialog.close();\n    });\n    dialog.addEventListener('keydown', e => {\n        if (e.key === 'Escape') dialog.close();\n    });\n    function escapeHtml(unsafe) {\n        return unsafe\n            .replace(/&/g, \"&amp;\")\n            .replace(/<\/g, \"&lt;\")\n            .replace(/>/g, \"&gt;\")\n            .replace(/\"/g, \"&quot;\")\n            .replace(/'/g, \"&#039;\");\n    }\n\n    function make_post_loader(){\n        let resp_json = null;\n        let to_load = 0;\n        const load_n = 10;\n\n        function loader(){\n            to_load = to_load + 1\n            if(!resp_json){\n                resp_json = fetch('https://mastodon.social/api/v1/statuses/109944483497018229/context').\n                            then(function(response){ return response.json() });\n            }\n\n            resp_json.then(function (data){\n                if(data['descendants'] &&\n                   Array.isArray(data['descendants']) && \n                   data['descendants'].length > 0) {\n                    document.getElementById('mastodon-comments-list').innerHTML = \"\";\n                    data['descendants'].slice(0, load_n * to_load).forEach(function(reply) {\n                        reply.account.display_name = escapeHtml(reply.account.display_name);\n                        reply.account.emojis.forEach(emoji => {\n                            reply.account.display_name = \n                                reply.account.display_name.replace( `:${emoji.shortcode}: `,\n                                                                    `<img src=\"${escapeHtml(emoji.static_url)}\" alt=\"Emoji ${emoji.shortcode}\" height=\"20\" width=\"20\" /> `);\n                        });\n                        mastodonComment =\n                            `<div class=\"mastodon-comment\">\n  <div class=\"avatar\">\n    <img src=\"${escapeHtml(reply.account.avatar_static)}\" height=60 width=60 alt=\"\">\n    <div>\n      <div class=\"content\">\n        <div class=\"author\">\n          <a href=\"${reply.account.url}\" rel=\"nofollow\">\n            <span>${reply.account.display_name}<\/span>\n            <span class=\"disabled\">${escapeHtml(reply.account.acct)}<\/span>\n          <\/a>\n          <a class=\"date\" href=\"${reply.uri}\" rel=\"nofollow\">\n            ${reply.created_at.substr(0, 10)}\n          <\/a>\n        <\/div>\n        <div class=\"mastodon-comment-content\">${reply.content}<\/div> \n      <\/div>\n    <\/div> `;\n                        document.getElementById('mastodon-comments-list').appendChild(DOMPurify.sanitize(mastodonComment, {'RETURN_DOM_FRAGMENT': true}));\n                        document.getElementById('load-comment').innerText = \"Load more\"\n                    });\n                } else {\n                    document.getElementById('mastodon-comments-list').innerHTML = \"<p>No comments found<\/p>\";\n                }\n            }); \n        }\n        return loader;\n    }\n\n    document.getElementById(\"load-comment\").addEventListener(\"click\", make_post_loader());\n\n\n  (function(){\n    document.getElementsByClassName(\"d-title\")[0].getElementsByTagName(\"h1\")[0].innerHTML = \"Reproducible medical imaging software environments in Nix.<\/br> or<\/br> Living in the future is hard.\"; \n  })();\n\na lot of ink has been spilled detailing how nix works and why you should use it. My treatment will be relatively superficial, instead focusing on my efforts and challenges using it in practice. If you are interested in the former I recommend:\n- https://serokell.io/blog/what-is-nix\n- https://revelry.co/insights/development/nix-time/ â†©ï¸Ž\nspecifically a nix flake, something like a library or package, useful for combining different projects that use nixâ†©ï¸Ž\nwell, most anywayâ†©ï¸Ž\nalthough it is recommended you manage julia packages not using nix.â†©ï¸Ž\nI will probably switch to or add docker before longâ†©ï¸Ž\nincluding pytorch and tensorflow, let it run overnight. This is mostly due to CUDA, I explain a bit laterâ†©ï¸Ž\nNix can mean a lot of things: https://www.haskellforall.com/2022/08/stop-calling-everything-nix.htmlâ†©ï¸Ž\nonce youâ€™ve conquered learning it.â†©ï¸Ž\nthis is also a pain point, which Iâ€™ll talk about later. But itâ€™s painful only because nix allows you to do something that essentially no one would even try to do with other package managers.â†©ï¸Ž\nit does this by patching the produced compiled artifacts to point to their â€œdynamicâ€ dependencies statically using @rpathâ†©ï¸Ž\nto be fair, pypi doesnâ€™t have package quality standards, many of these are abandoned or malwareâ†©ï¸Ž\nby introducing possible system dependencies and portability issues, although I would argue incrementally better is still better, a few risky packages is better than all risky packages.â†©ï¸Ž\nalthough hopefully returning as part of https://github.com/nix-community/dream2nix in the futureâ†©ï¸Ž\n a colleague suggests that we create a build farm where a background process builds new versions of things so that there are prebuilt versions always available, but that is an infrastructure investment that hasnâ€™t felt worth it yet. Apparently the issue is that CUDA is technically not free software and by policy the nix build farms wonâ€™t build it for us. â†©ï¸Ž\nyou need to start programs with nixGL<driver> <program> which is irritating, but I think can be fixed in my flakeâ†©ï¸Ž\nOk youâ€™d need to edit one line in the flake but thatâ€™s enough of a barrier to discourage users. If anyone in the nix community can help me solve this problem Iâ€™d be extremely gratefulâ†©ï¸Ž\nand of course contributing back to nixpkgs where you canâ†©ï¸Ž\n",
    "preview": "posts/2023-02-28-medical-imaging-with-nix/future-compute-v2.jpg",
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-01-27-with-a-little-help-from-my-friends/",
    "title": "With A Little Help From My Friends",
    "description": "Accompanying blog post for the Posit Community talk: \"With A Little Help From My Friends: Tools and insights for developing and deploying in the hospitalâ€.",
    "author": [
      {
        "name": "Chloe Pou-Prom",
        "url": {}
      }
    ],
    "date": "2023-01-27",
    "categories": [
      "post-miscellaneous",
      "post-talks"
    ],
    "contents": "\n\nThis is a summary of â€œWith A Little Help From My Friends: Tools and insights for developing and deploying in the hospitalâ€, a talk given on November 15, 2022 as part of the Posit Enterprise Community Meetup series. A recording of the talk is available here and a PDF of the accompanying slides can be found here.\n\nA success story\nHere at DSAA, we developed CHARTwatch, an early warning system for detecting patient deterioration. The system runs every hour:\nFirst, CHARTwatch ingests laboratory values, vital measurements, and demographics.\nThen, CHARTwatch classifies each visit into the following group: High risk vs Medium risk vs Low risk.\nThe risk group predictions are then delivered through different methods:\nEmails\nUpdates to a front-end tool\nAlerts sent to phones\n\nFinally, on August 2020, CHARTwatch went live and we were able to deploy to the General Internal Medicine (GIM) ward!! Developing and deploying CHARTwatch was a difficult feat and wouldnâ€™t have been possible without the help of a few friendsâ€¦\nTools for development\nDatabase connections\nThere are various data systems in the hospital, all with their own quirks and intricacies. In order to make it easier for the Advanced Analytics team to work with the different hospital databases, we developed chartdb, an internal R package.\nThe connection functions follow the same pattern, making it easier for data scientists and data analysts to work with databases.\n\n\ncon_a <- chartdb::connect_databaseA(username = ..., password = ...)\ncon_b <- chartdb::connect_databaseB(username = ..., password = ...)\ncon_edw <- chartdb::connect_edw(username = ..., password = ...)\ncon_soarian <- chartdb::connect_soarian(username = ..., password = ...)\ncon_mak <- chartdb::connect_mak(username = ..., password = ...)\ncon_syngo <- chartdb::connect_syngo(username = ..., password = ...)\n\n\n\nA reproducible environment\n\nA one-sentence horror story: â€œIt works on my machine.â€\n\n\nTo improve reproducibility, we use renv, an R package for R dependency management.\nWhen setting up your renv environment, the renv.lock will keep track of the different packages (and their versions!!). If a new person needs to work on the project, they can use the renv.lock file to download the same packages and the same package versions! Woohoo!\nPackage-based development\nWhy write a package?\nPlenty! Of! Reasons!\nIn particular, writing a package makes it easy to share code and knowledge with others.\nAnd, writing a package greatly reduces the amount of times youâ€™re copy-pasting code. Really, itâ€™s a win-win-win situation!\nEnvironment! Environment! Environments\nWe typically work with 3 different environment.\nThe development environment is the one on your local computer or your development server.\nThe staging environment is as close to the â€œrealâ€ deployment environment as possible.\nThe production environment is where things actually get deployed. Speaking of production environments, nowâ€™s a great time to talk about the tools we need for deployment. Before doing that though, time forâ€¦\nAn interlude\nA multi-sentence horror story:\n\nIn earlier attempts to deploy CHARTwatch, we were using multiple CRON jobsâ€¦ that called different bash scriptsâ€¦ which then called different Python/R/Java scripts. Oh, and we had no separate environments. Or rather, we only had one environment, where test environment = staging environment = production environment\n\n\nTools for deployment\nInterlude over! Letâ€™s looks the tools we need for deployment.\nAuthentication\n\nWe rely on Posit Connect for this. Posit Connect works with the hospital Active Directory, which enables administrators to manage permissions and access. What does this mean?\nDevelopers donâ€™t need to keep track of an extra server username and password.\nEnd-users can log in with their hospital credentials!\nScheduling\nPosit Connect also gives us the ability to schedule scripts through their admin interface.\nDowntime\nKnowing when things donâ€™t work is crucial for deployment.\nWe use jarvis, our own internally-developed R package, to email and Slack notifications alerting us of issues.\nEach project will also have downtime protocol which defines the steps that various team members must take when something isnâ€™t working.\nFor example, when a CHARTwatch downtime affects end-users, an email alert is sent to the entire hospital. Not all projects will require something like this, but since CHARTwatch is embedded in clinical care, many need to be aware of the downtime.\n\nA secure way to download internally-developed packages\nWe need to limit who can access the hospital network. Posit Package Manager is a repository package management that lets us download packages while being disconnected from the Internet.\nAn implementation plan\nDeploying an early warning system in the hospital involves working with many different groups. All worked together to develop the implementation plan. Some takeaways from the implementation plan:\nConsider existing resources! Thereâ€™s no point in re-implementing a workflow that already exists.\nFor example, notifications were designed so that they fit within existing processes. Emails to charge nurses were sent at times that made the most sense based on their shifts.\n\nHave a silent deployment period! This is useful for identifying unexpected bugs.\nOne bug we found in silent deployment was caused by a particular NAsty labâ€¦\n\nFind out more\nThis is a very very very very brief overview of CHARTwatch and how we develop and deploy models in the hospital. Check out the reading list below for more details.\nTo learn more about CHARTwatchâ€™s model development and validation, see: â€œPreparing a Clinical Support Model for Silent Mode in General Internal Medicineâ€ by Nestor et al.Â (2020).\nTo learn more about the process changes that were required for CHARTwatchâ€™s implementation, see: â€œImplementing Machine Learning in Medicineâ€ by Verma et al.Â (2021)\nTo read all about the technical infrastructure that lead to CHARTwatchâ€™s deployment, see: â€œFrom Compute to Care: Lessons Learned from Deploying an Early Warning System into Clinical Practiceâ€ by Pou-Prom et al.Â (2022)\nTo find out all about the data that is used to train CHARTwatch, see: â€œGIM, a dataset for predicting patient deterioration in the General Internal Medicine ward (version 1.0.0)â€ by Kuzulugil et al.Â (2022)\nThis white paper by Signal1: â€œThe Burden of Clinical Deterioration and How One Hospital is Tackling it with Machine Learningâ€\n\n\n\n",
    "preview": "posts/2023-01-27-with-a-little-help-from-my-friends/this_is_fine.png",
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-16-dsaa-wrapped/",
    "title": "DSAA Wrapped",
    "description": "Reflecting on 2022. [5 min read]",
    "author": [
      {
        "name": "Chloe Pou-Prom, Meggie Debnath, Derek Beaton, and Cathy Zhu on behalf of DSAA",
        "url": {}
      }
    ],
    "date": "2022-12-13",
    "categories": [
      "post-miscellaneous"
    ],
    "contents": "\nThe end of the year is a time for reflection, reminiscing, and sharing screenshots of our Spotify Wrapped. For those who are unfamiliar, Spotify Wrapped is a yearly marketing campaign by Spotify, where they compile streaming data and create a personalized graphic for each user. 1\nIn the spirit of data, end-of-year reflections, and colorful infographics, we thought it would be fun to create our very own DSAA wrapped!\nEnjoy some stats!!\n\n\n\nProjects\n2022 was a busy year with lots of projects!!\nTotal # of projects deployed: 9\nMost viewed project: Ops Centre\nThe Ops Centre project provides a clear picture of patient flow within the hospital. It consists of multiple data tiles that reflect real-time data in the areas of occupancy, upcoming discharges, planned admissions, bed cleaning, portering, and other elements essential to providing a full-picture of patient flow.\n\nProject with the most team members whose names start with M: Hemodialysis\nThe Hemodialysis project is a weekly digest tool for clinicians at the St.Â Michaelâ€™s Hospital hemodialysis clinics. The tool provides patientsâ€™ risks of a unplanned hospital visits.\n\nProduct families\n2022 was the year of product families.2\n\nWhoa, wait. What exactly is a product family?\nProduct at DSAA is a platform, application, system, or report that is created to meet patient, clinician, or hospital operational needs.\nProduct family (roughly comparable to a product line) is a group of related products that are developed and maintained by a single cross-functional team at DSAA.\nWe adopted this custom framework in order to allow staff to work effectively within persistent teams and to leverage resources across products the best we can. The products under each product family are related by common product value and/or shared domain expertise requirement.\nSome product family stats!\nNumber of product families: 6\nProduct family with the most projects: The Patient Flow product family 3\nThe Patient Flow product family aims to make the patients journey though the hospital as safe and comfortable as possible without compromising care. The Ops Centre is an example of a project that is part of this family.\n\nDSAA and improvement\n2022 was a year where we focused on improving existing operations.\n# of sprint improvement weeks: 3\nThe sprint improvement week is a full week dedicated to DSAA process improvement work in the areas of data infrastructure, source system optimization, DevOps, data governance, databases, analytics and project management processes, and more! The DSAA team works together to clear a backlog of tasks, and then we share our accomplishments through demos at the end of the week.\nSome notable achievements from past sprint improvement weeks are: our very own DSAA handbook, maintenance and update of internally-developed packages, and experimenting with containers!\n\n# of blog posts: 5\n2022 is when we re-launched this blog!\n\n# of journal club sessions lead by the Advanced Analytics team: at least 16\nWe covered the following topics: AI ethics, equity, & inclusion, anomaly detection4, survival analysis, Canadian healthcare.\n\n# of â€œsheetsâ€ created: 35\nTemplates! Templates! Templates! We worked on templates for: Datasheet, Factsheet, and Models card sheet.\n\n# of Advanced Analytics mini-demos: 8\nMini-demos are lead by a different Advanced Analytics team member and include (you guessed it) a mini-demo!! Usually, one of the team members will try out a cool new package or talk about a fun/interesting/challenging problem they ran into. For example, we had a mini-demo on the funneljoin package.\n\nDSAAâ€™s social committee\nOur teamâ€™s social committee (a.k.a. the Party Planning Committee) (a.k.a. the Party Planning Pandas) have organized some GREAT events this year.\n# of steps completed during the Step Challenge: 1,983,000\nThe Step Challenge was 1-week challenge in May where team members were encouraged to get as many steps in as possible! The max # of daily steps was 32,998 and the min # of daily steps was 246.\n\n# of â€œwizardâ€ classes: 3\nWizard classes are DSAA-lead classes to learn something fun. This year we learned how to bake puff pastries, how to play euchre and how to make pizza.\nSpeaking of pizzaâ€¦.\n\n# of pizzas made by DSAA team members: \\(\\infty\\)\nPizza lesson night was very popular!!! Check out some of delicious pizzas that team members made:\n\n\n\nMost requested album: The Charlie Brown Christmas soundtrack\nDuring the year, many of our team members would hop on a Zoom call, play some music, and work together. The most played album was the Charlie Brown Christmas soundtrack, very closely followed by lo-fi chill hip hop.\n\nâ€¦ and thatâ€™s a wrap!\n2022 was a great year and weâ€™re looking forward to 2023.\nSome things you can expect in the new year:\nMore blog posts!!!!!!!!6\nMore pizza!\nMore journal club sessions. Weâ€™re looking forward to reading up on NLP and timeseries!\nMore steps!\n\nWe surveyed the members on DSAA and found that the most listened music genre is Pop, followed by various interesting variations of Indie (indie pop, indie soul, indie QuÃ©bÃ©cois).â†©ï¸Ž\nBlog post coming soon!! ðŸ¤žâ†©ï¸Ž\nFun fact: The Patient Flow product family is also the product family with the most Fast & Furious memes.â†©ï¸Ž\nblog post coming soon! :-)â†©ï¸Ž\nBlog post coming soon!!! ðŸ™â†©ï¸Ž\nWe promised a few in this blog post, so now weâ€™re on the hook!â†©ï¸Ž\n",
    "preview": "posts/2022-12-16-dsaa-wrapped/dsaa_wrapped_preview.PNG",
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-12-putting-the-fun-in-funneljoin/",
    "title": "Putting the fun in `funneljoin`",
    "description": "An introduction to, an example of, and my (lazy) journey to discovering the `funneljoin` package. [10 min read]",
    "author": [
      {
        "name": "Derek Beaton",
        "url": "https://twitter.com/derek__beaton"
      }
    ],
    "date": "2022-08-12",
    "categories": [
      "language-R",
      "project-chartwatch",
      "post-miscellaneous",
      "funneljoin"
    ],
    "contents": "\n\nContents\nMy lazy journey to discovery\nA quick view of the data\nA look at the data\nWhat we want and the behavior weâ€™d expect\n\nA journey to find and a journey through funneljoin\nMy first failure\nMy second failure\nA third successful attempt\n\nConclusions\n\nMy lazy journey to discovery\nA while back, I was helping1 out on one of our new projects: an expansion of CHARTWatch to new units. To learn a bit more about CHARTWatch you should read (Verma et al. 2021) that explains how to get models in the clinical environment and (Pou-Prom et al., n.d.) on all the technical parts required.\nWe were doing some data exploration for the project and we had a fairly straight forward question to answer: What is the first event after a procedure for a patient?\nWith that question, I dove into the data files we had started to get a feel for what to do2. I had a few files to work with, lots of timestamps all over the place, and knew I had some sort of join type problem. I triedâ€”and failed withâ€”many variations of joins (e.g., fuzzyjoin) and even trying to get what was needed in more manual3 ways. One thought kept ringing in my head: â€œsomeone must have solved this problem.â€ So at that point I spent a few half days4 searching. Many fruitless paths later and on the verge of the more manual5 approaches I finally found what I was looking for: funneljoin.\nThough funneljoin was what I was looking for, my use of it was also a journey through multiple mistakes and misunderstandings all of which are my own6. But I eventually got exactly what I wanted: a straightforward way to join some data and find very specific events that occur after other events.\nA quick view of the data\nLetâ€™s start out by taking a look at the data. Thatâ€™ll give us a better sense of the problem and the behavior weâ€™re expecting. Weâ€™re going to be working with a tiny example of what the real data could look like. These fake data have been created from real data and then we used uuid, dplyr::group_by, lubridate and some good old fashioned randomization to make it fake.\nWe have two data files which look a lot like our real data:\nALL_ADT_EVENTS.csv: A file that contains all the Admit-Discharge-Transfer (ADT) events for patients while they are in the hospital, and\nSPECIFIC_PROCEDURES.csv: which contains a very specific set of procedures while in the hospital.\nItâ€™s very worth noting that all the events in SPECIFIC_PROCEDURES are in ALL_ADT_EVENTS. These are separate for a few reasons including (but certainly not limited to): itâ€™s easier to work with when we want to know only about the procedures, and things like procedures can (and are) pulled from separate pipelines more specific than ADT pipelines.\nA look at the data\nLetâ€™s start out by taking a look at some of the ADT file and a few (preselected) rows to highlight these data. And while weâ€™re at it, weâ€™ll see the code, too!\n\n\nlibrary(here)       ## for here::here() and referencing files from this .RProj\nlibrary(dplyr)      ## for some processing and those fancy pipes\nlibrary(kableExtra) ## for some extra fancy looking tables\nlibrary(rmarkdown)  ## for some even more extra fancy tables\n\nADT_FILE_PATH <- here::here(\"_posts\",\"2022-08-12-putting-the-fun-in-funneljoin\", \"ALL_ADT_EVENTS.csv\")\nADT_EVENTS <- read.csv(ADT_FILE_PATH, \n                        stringsAsFactors = FALSE)\n\nADT_EVENTS %>%\n  slice( c(1, 2, 3, 12, 13, 16, 20, 21) ) %>%\n  kableExtra::kbl() %>%\n  kableExtra::kable_styling() \n\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED\n\n\nFROM_SERVICE\n\n\nTO_SERVICE\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-05-10 06:08:10\n\n\nNA\n\n\nED TRIAGE\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-05-10 06:13:24\n\n\nED TRIAGE\n\n\nED TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-05-10 06:54:10\n\n\nED TRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-13 11:32:48\n\n\nINTENSIVE CARE TRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-18 19:53:13\n\n\nINTENSIVE CARE TRAUMA\n\n\nTRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-28 02:14:32\n\n\nINTENSIVE CARE TRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-29 22:09:10\n\n\nINTENSIVE CARE TRAUMA\n\n\nNA\n\n\nIn our table we see some (preselected) rows and all of our columns. Weâ€™re looking at just 1 patient (ENCOUNTER_NUM_ANONYMIZED) with a snapshot of some of their events (EVENT_TS_FUZZED), which service they were coming from (FROM_SERVICE) and which service they were going to (TO_SERVICE).\nLetâ€™s now take a look at the SPECIFIC_PROCEDURES data\n\n\nPROCEDURE_FILE_PATH <- here::here(\"_posts\",\"2022-08-12-putting-the-fun-in-funneljoin\", \"SPECIFIC_PROCEDURES.csv\")\nSPECIFIC_PROCEDURES <- read.csv(PROCEDURE_FILE_PATH,\n                                 stringsAsFactors = FALSE)\n\nSPECIFIC_PROCEDURES %>%\n  kableExtra::kbl() %>%\n  kableExtra::kable_styling() \n\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED\n\n\nFROM_SERVICE\n\n\nTO_SERVICE\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 21:34:13\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\n7cc7972a-58a1-4c78-a0dc-83021c6dc0c6\n\n\n2083-06-02 10:46:45\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\n7cc7972a-58a1-4c78-a0dc-83021c6dc0c6\n\n\n2083-06-22 02:59:57\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\n87d439b0-4f26-4b73-8df3-bf5dbbf34ca7\n\n\n2090-09-05 15:01:52\n\n\nNEUROSURGERY\n\n\nINTENSIVE CARE NEURO SURGERY\n\n\nWeâ€™re showing all of SPECIFIC_PROCEDURES because itâ€™s much smaller. It has the same structure as the ADT file (and thatâ€™s because the procedures are a subset of all the ADT events). Now that we see the procedures we can also see that itâ€™s an event in ADT_EVENTS. The first SPECIFIC_PROCEDURE:\n\n\nSPECIFIC_PROCEDURES %>%\n  slice(1) %>%\n  kableExtra::kbl() %>%\n  kableExtra::kable_styling() \n\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED\n\n\nFROM_SERVICE\n\n\nTO_SERVICE\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\nand now the ADT events with the event before, the procedure event, and the event after (which is what we want to specifically identify; eventually that is).\n\n\nADT_EVENTS %>%\n  slice( c(15, 16, 17) ) %>%\n  kableExtra::kbl() %>%\n  kableExtra::kable_styling() \n\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED\n\n\nFROM_SERVICE\n\n\nTO_SERVICE\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-18 20:02:58\n\n\nTRAUMA\n\n\nTRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 19:54:10\n\n\nINTENSIVE CARE TRAUMA\n\n\nTRAUMA\n\n\nA fun note before we move on: When you look at these data youâ€™ll see some NA in there. Those NA are Rs NA which is effectively missing data. In this case NA is absolutely not NA which is sodium (sodium is not a service). You should really take a look at some of the â€œfunâ€ with NA weâ€™ve had.\nWhat we want and the behavior weâ€™d expect\nOur task was to identify the event that happens after specific procedures. From the above, we can see an example of that:\n\n\nADT_EVENTS %>%\n  slice( c(16, 17) ) %>%\n  kableExtra::kbl() %>%\n  kableExtra::kable_styling() \n\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED\n\n\nFROM_SERVICE\n\n\nTO_SERVICE\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 19:54:10\n\n\nINTENSIVE CARE TRAUMA\n\n\nTRAUMA\n\n\nThe first row is the procedure and the second row is the event that happens after a specific procedure. At the end of what we do, we want all of those events after procedures because we needed to understand more about patient movements after procedures for some of our modelling.\nA journey to find and a journey through funneljoin\nGiven that we have two data sets and we know that we want to match on certain things (ENCOUNTER_NUM_ANONYMIZED) but conditional on subsequent time stamps (EVENT_TS_FUZZED), we probably have some sort of join problem.\nI spent a lot of time trying out a lot of the standard join and merge options we find in R: dplyr, base::merge, and even ventured off into the land of fuzzyjoin. I tried a lot of things and all of those things were wrong or overly complicated.\nSo instead of just writing some code to find the next line in the ADT events data after a matching line in the procedures data, I spent a few7 half days8 searching for a package that probably does the thing Iâ€™m looking for. It took me quite a while and a variety of search terms (e.g., â€œtime series join,â€ â€œjoin events after,â€ â€œfuzzyjoin for time,â€ â€œwhy doesnâ€™t this specific thing I want exist and why am I so bad at this?â€) until I eventually found funneljoin9.\nThe funneljoin package includes a lot of join options for time series data. In particular after_join is when weâ€™re looking for events in one data set that occur after events in another data set. That was exactly what I was looking for. Though after_join was the key to solving my problems, I still had a few more problems10 but did eventually figure it all out and it was magical. Letâ€™s walk through three examples with these data and after_join to show where I failed and where I eventually succeeded.\nMy first failure\nI dove into after_join with, basically, the default parameters. So letâ€™s break this down:\nx is our procedures data because thatâ€™s our reference point: we want ADT events after procedures\ny is our ADT data to get those events after procedures\nby_user is the column we use for identifiers (ENCOUNTER_NUM_ANONYMIZED); this is a very typical parameter to expect in join and merge, as we usually want to join data sets based on some identifier\nby_time is the column we use to find the events (EVENT_TS_FUZZED); this is how funneljoin makes use of time series data\nsuffix appends labels to the newly created columns after the join, respectively for x then y\nSeems straight forward enough! Letâ€™s see what happens:\n\n\nlibrary(funneljoin)\n\njoin_attempt_one <- after_join(\n  x = SPECIFIC_PROCEDURES,\n  y = ADT_EVENTS,\n  by_user = \"ENCOUNTER_NUM_ANONYMIZED\",\n  by_time = \"EVENT_TS_FUZZED\",\n  suffix = c(\"_PROCEDURES\",\"_ADT\")\n)\n\nrmarkdown::paged_table(join_attempt_one)\n\n\n\n\n{\"columns\":[{\"label\":[\"ENCOUNTER_NUM_ANONYMIZED\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"EVENT_TS_FUZZED_PROCEDURES\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"FROM_SERVICE_PROCEDURES\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"TO_SERVICE_PROCEDURES\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"EVENT_TS_FUZZED_ADT\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"FROM_SERVICE_ADT\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"TO_SERVICE_ADT\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\nErrâ€¦ it sort of looks like nothing happens. Why is that? Well itâ€™s because our resulting join is empty11\nMy second failure\nI clearly missed something12. On a closer look at the parameters, it seemed like I needed a couple of more to make things work. So I brought them in and tried:\nmode is inner which is a specific type of join which is basically the intersection\ntype here is one of many options on how to think about the join and we set it to first-firstafter\nBefore we dive into the updated code letâ€™s pause for what type is and what first-firstafter means. Heavily borrowing from the funneljoin site:\nfirst-firstafter: Take the first x, then the first y after that. For example, we have the first procedure for the first patient in the procedures (x) data, and we want the first event from the ADT (y) data that occurs afterwards. We donâ€™t want all afterward, we donâ€™t want any before. Just the one! So letâ€™s try it:\n\n\njoin_attempt_two <- after_join(\n  x = SPECIFIC_PROCEDURES,\n  y = ADT_EVENTS,\n  by_user = \"ENCOUNTER_NUM_ANONYMIZED\",\n  by_time = \"EVENT_TS_FUZZED\",\n  suffix = c(\"_PROCEDURES\",\"_ADT\"),\n  mode = \"inner\",\n  type = \"first-firstafter\"\n)\n\nrmarkdown::paged_table(join_attempt_two)\n\n\n\n\n{\"columns\":[{\"label\":[\"ENCOUNTER_NUM_ANONYMIZED\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"EVENT_TS_FUZZED_PROCEDURES\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"FROM_SERVICE_PROCEDURES\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"TO_SERVICE_PROCEDURES\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"EVENT_TS_FUZZED_ADT\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"FROM_SERVICE_ADT\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"TO_SERVICE_ADT\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"a4461971-22e1-46e1-8454-aa7fbce54205\",\"2\":\"2082-06-22 10:27:06\",\"3\":\"TRAUMA\",\"4\":\"INTENSIVE CARE TRAUMA\",\"5\":\"2082-06-22 10:27:06\",\"6\":\"TRAUMA\",\"7\":\"INTENSIVE CARE TRAUMA\"},{\"1\":\"7cc7972a-58a1-4c78-a0dc-83021c6dc0c6\",\"2\":\"2083-06-02 10:46:45\",\"3\":\"TRAUMA\",\"4\":\"INTENSIVE CARE TRAUMA\",\"5\":\"2083-06-02 10:46:45\",\"6\":\"TRAUMA\",\"7\":\"INTENSIVE CARE TRAUMA\"},{\"1\":\"87d439b0-4f26-4b73-8df3-bf5dbbf34ca7\",\"2\":\"2090-09-05 15:01:52\",\"3\":\"NEUROSURGERY\",\"4\":\"INTENSIVE CARE NEURO SURGERY\",\"5\":\"2090-09-05 15:01:52\",\"6\":\"NEUROSURGERY\",\"7\":\"INTENSIVE CARE NEURO SURGERY\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\nThe above shows the whole table but letâ€™s take a closer look at somethingâ€¦\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED_PROCEDURES\n\n\nEVENT_TS_FUZZED_ADT\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\n2082-06-22 10:27:06\n\n\n7cc7972a-58a1-4c78-a0dc-83021c6dc0c6\n\n\n2083-06-02 10:46:45\n\n\n2083-06-02 10:46:45\n\n\n87d439b0-4f26-4b73-8df3-bf5dbbf34ca7\n\n\n2090-09-05 15:01:52\n\n\n2090-09-05 15:01:52\n\n\nWell that didnâ€™t quite work because itâ€™s actually finding the same time stamped events. So weâ€™re not yet finding the first event after but weâ€™ve at least got something. Soâ€¦ what are we missing?\nA third successful attempt\nWhat weâ€™re missing is the next event. Right now, weâ€™re getting back the same event. Fortunately thereâ€™s a parameter for that:\nmin_gap allows us to specify how much time there must be in between the first-firstafter events. There are also two companion parameters to this: max_gap and gap_col which tell us, respectively, the maximum time between events and a column to include the gap in time itself. For fun, letâ€™s also add in gap_col so we can see the amount of time between events.\n\n\njoin_attempt_three <- after_join(\n  x = SPECIFIC_PROCEDURES,\n  y = ADT_EVENTS,\n  by_user = \"ENCOUNTER_NUM_ANONYMIZED\",\n  by_time = \"EVENT_TS_FUZZED\",\n  suffix = c(\"_PROCEDURES\",\"_ADT\"),\n  mode = \"inner\",\n  type = \"first-firstafter\",\n  min_gap = base::as.difftime(1,units=\"secs\"),\n  gap_col = TRUE\n)\n\nrmarkdown::paged_table(join_attempt_three)\n\n\n\n\n{\"columns\":[{\"label\":[\"ENCOUNTER_NUM_ANONYMIZED\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"EVENT_TS_FUZZED_PROCEDURES\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"FROM_SERVICE_PROCEDURES\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"TO_SERVICE_PROCEDURES\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\".gap\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"EVENT_TS_FUZZED_ADT\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"FROM_SERVICE_ADT\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"TO_SERVICE_ADT\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"a4461971-22e1-46e1-8454-aa7fbce54205\",\"2\":\"2082-06-22 10:27:06\",\"3\":\"TRAUMA\",\"4\":\"INTENSIVE CARE TRAUMA\",\"5\":\"34024\",\"6\":\"2082-06-22 19:54:10\",\"7\":\"INTENSIVE CARE TRAUMA\",\"8\":\"TRAUMA\"},{\"1\":\"7cc7972a-58a1-4c78-a0dc-83021c6dc0c6\",\"2\":\"2083-06-02 10:46:45\",\"3\":\"TRAUMA\",\"4\":\"INTENSIVE CARE TRAUMA\",\"5\":\"1222979\",\"6\":\"2083-06-16 14:29:44\",\"7\":\"INTENSIVE CARE TRAUMA\",\"8\":\"INTENSIVE CARE TRAUMA\"},{\"1\":\"87d439b0-4f26-4b73-8df3-bf5dbbf34ca7\",\"2\":\"2090-09-05 15:01:52\",\"3\":\"NEUROSURGERY\",\"4\":\"INTENSIVE CARE NEURO SURGERY\",\"5\":\"74931\",\"6\":\"2090-09-06 11:50:43\",\"7\":\"INTENSIVE CARE NEURO SURGERY\",\"8\":\"NEUROSURGERY\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  \n\nOh that looks like we did it! Letâ€™s look at just the snapshot of encounter numbers and timestamps with the .gap column:\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED_PROCEDURES\n\n\nEVENT_TS_FUZZED_ADT\n\n\n.gap\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\n2082-06-22 19:54:10\n\n\n34024\n\n\n7cc7972a-58a1-4c78-a0dc-83021c6dc0c6\n\n\n2083-06-02 10:46:45\n\n\n2083-06-16 14:29:44\n\n\n1222979\n\n\n87d439b0-4f26-4b73-8df3-bf5dbbf34ca7\n\n\n2090-09-05 15:01:52\n\n\n2090-09-06 11:50:43\n\n\n74931\n\n\nOH I THINK WE REALLY DID DO IT. Letâ€™s just take a quick look back at an earlier chunk of code and verify based on just the ADT file for just one example13\n\n\nADT_EVENTS %>%\n  slice( c(16, 17) ) %>%\n  kableExtra::kbl() %>%\n  kableExtra::kable_styling() \n\n\n\nENCOUNTER_NUM_ANONYMIZED\n\n\nEVENT_TS_FUZZED\n\n\nFROM_SERVICE\n\n\nTO_SERVICE\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 10:27:06\n\n\nTRAUMA\n\n\nINTENSIVE CARE TRAUMA\n\n\na4461971-22e1-46e1-8454-aa7fbce54205\n\n\n2082-06-22 19:54:10\n\n\nINTENSIVE CARE TRAUMA\n\n\nTRAUMA\n\n\nConclusions\nThe first conclusion is that funneljoin is awesome and the second conclusion is that Iâ€™m lazy. It is also a safe assumption to conclude that Iâ€™ve used an excessive amount of unnecessary footnotes14.\nWhen we look back at these data and the problem, we probably could have solved this with some dplyr::group_by at the encounter (ID) level, do some checks on the timestamps, and some stuff like that. But we had these two data setsâ€”each used separately for different reasons in the same projectâ€”so why not make this easy and intuitive? Thatâ€™s the major advantage of funneljoin here.\nMaybe the way I solved this problem with funneljoin could have been better15, but this was really useful for me. It was also a super useful exercise for us to find this package and start to understand it because we deal with a lot of time stamps. We often have frequent questions about order of events, or we have to pull multiple pieces of data together from a variety of source systems and ensure itâ€™s all in the right order.\nAnd learning funneljoin was fun and you can tell because itâ€™s literally in the name.\n\n\n\nPou-Prom, ChloÃ©, Joshua Murray, Sebnem Kuzulugil, Muhammad Mamdani, and Amol Verma. n.d. â€œFrom Compute to Care: Lessons Learned from Deploying an Early Warning System into Clinical Practice.â€ Frontiers in Digital Health, 174. https://www.frontiersin.org/articles/10.3389/fdgth.2022.932123/abstract.\n\n\nVerma, Amol A., Joshua Murray, Russell Greiner, Joseph Paul Cohen, Kaveh G. Shojania, Marzyeh Ghassemi, Sharon E. Straus, Chloe Pou-Prom, and Muhammad Mamdani. 2021. â€œImplementing Machine Learning in Medicine.â€ CMAJ 193 (34): E1351â€“57. https://doi.org/10.1503/cmaj.202434.\n\n\nâ€œhelpingâ€ is generous: I mostly asked a million stupid questions and as weâ€™ve seen, spent many days finding a package to do what I want instead of just doing itâ†©ï¸Ž\nagain: not without asking a million stupid questions and getting back a million fantastic answersâ†©ï¸Ž\nand very hackyâ†©ï¸Ž\na â€˜fewâ€™ is defined as a week and â€˜half dayâ€™ is defined as â€˜definitely more than half a dayâ€™â†©ï¸Ž\nand very hackyâ†©ï¸Ž\na million stupid questions followed by a million stupid mistakesâ†©ï¸Ž\nagain: not a fewâ†©ï¸Ž\nagain: definitely more than half daysâ†©ï¸Ž\nfound is a generous term, I more so stumbled across it with no recollection how I found it after a weekâ†©ï¸Ž\nDid I mention Iâ€™m bad at this?â†©ï¸Ž\nagain: I ainâ€™t so good at thisâ†©ï¸Ž\nthis is a generous way of saying I didnâ€™t read the documentationâ†©ï¸Ž\nwe really should verify for all but have I mentioned Iâ€™m lazy?â†©ï¸Ž\nnot enough footnotes and they are totally necessaryâ†©ï¸Ž\nalmost certainly it could have been betterâ†©ï¸Ž\n",
    "preview": "posts/2022-08-12-putting-the-fun-in-funneljoin/preview.png",
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-04-posits-conf-time/",
    "title": "POS...IT's conf() time ðŸ¥",
    "description": "Pause, it's time to write about RStudio conf(2022). [10 min read]",
    "author": [
      {
        "name": "Maitreyee Sidhaye",
        "url": {}
      },
      {
        "name": "Chloe Pou-Prom",
        "url": {}
      }
    ],
    "date": "2022-08-04",
    "categories": [
      "post-miscellaneous"
    ],
    "contents": "\n\nContents\n2 years before\n2 weeks before\nDuring\nDo not skip the keynotes!\nFocus on the content of the talk\nGet social!\nCreate a POSITive space\nThings no one will tell youâ€¦\n\n2 business days later\n2 months laterâ€¦?\n\n2 years before\nPicture this. Itâ€™s January 2020, youâ€™re in Toronto, itâ€™s the middle of winter, youâ€™re just back from vacationâ€¦ and youâ€™re suffering from FOMO1 because your teammates are in sunny San Francisco collecting hex stickers, taking selfies with Hadley, and having the time of their lives at rstudio::conf(2020).\nWe were jealous.\nFast-forwardâ€¦ and now WE are the ones living the dream! âœ¨\n\n\n\nFigure 1: Check out the view! Now we are the ones creating FOMO ðŸ˜Ž\n\n\n\n2 weeks before\nThis was the first time for both of us and we wanted to be ready. Hereâ€™s how you can do it too!\nDownload the RStudio conf app and bookmark all the talks that interest you. We asked our teammates which topics would be most relevant for the team. We ended up with the following: data science for healthcare, working with Quarto, working with Python, putting things in deployment/production, enhancements to tidymodels, best practices, MLOps/DevOps, training/teaching.\nWatch old talks. These can all be found here.2\nReach out to people who will be attending. The motto for this yearâ€™s conference was â€œItâ€™s always better when weâ€™re togetherâ€. Keeping that in mind, we reached out to our awesome RStudio customer success rep and to friends we had met through the RStudio Community Meetup. It was great to see them in-person!\nMake a list of places you want to visit. We prepared our sight-seeing and food map.\nDuring\nDo not skip the keynotes!\nWe got to experience our very own Oprah moment during the first keynote when the Posit announcement was made. Below is a sensationalized retelling of what happened during the very first talk.\n\n\nHADLEY, shortly after making the Posit rebranding announcement: â€œNow some of you may be wonderingâ€¦ where are our new stickers?â€\nHADLEY pauses and surveys the crowd with a mischievous look.\nHADLEY: Now, if you all take a look under your seatâ€¦\n\n\nIntense shuffling sounds can be heard as THE AUDIENCE begins to search under their seat. People gasp and laugh as they pull envelopes from under their seat. MAITREYEE and CHLOE take their envelopes and find two stickers!\n\n\n\nFigure 2: Sticker reveal!\n\n\n\n[LATER IN THE EVENINGâ€¦]\n\n\nIn the interest of avoiding widespread panic weâ€™re not moving away from hexagons for package stickers. posit isnâ€™t a package so it gets a different shape #rstudioconf\n\nâ€” Hadley Wickham (@hadleywickham) July 27, 2022\n\n\n\nFocus on the content of the talk\nWe didnâ€™t need to take extensive notes during the talks, since materials are available online.\nRStudio cares about the quality of the presentations. All speakers received coaching and it paid off. All the talks were engaging, easy-to-follow, and easy-to-digest.\nGet social!\nThere were plenty of opportunities for socializing and networking.\nThe evening reception was a great place to meet people over drinks, music, and games.\nThe R-ladies Meetup allowed us to meet awesome women that work with R, grab some hex stickers, and pick up the Women in STEM card deck.\n\n\nHello from #RLadies at #RStudioConf!! ðŸ‘‹#RStudioConf2022 pic.twitter.com/G9gqxpdnw7\n\nâ€” R-Ladies Global (@RLadiesGlobal) July 29, 2022\n\nThe Birds of a Feather groups provide an opportunity to meet people that work on the same problems or use the same tools as you. We attended the Healthcare Birds of a Feather group and got to meet plenty of data practitioners who offered us insights into how things work at other hospitals and health institutions.\nCreate a POSITive space\nRStudio is committed to keeping their events inclusive and welcoming, which is highlighted by the Pac-man rule. During the conference, their actions supported their words through the use of pronoun pins, a masking policy, the use of color-coded â€œsocial distanceâ€ pins3, and diversity scholarships.\nThings no one will tell youâ€¦\nGrab food from the dessert table before the main food table. We learned this the hard way. Desserts disappear as fast as holographic hex stickers.\nSpeaking of stickersâ€¦ make sure to grab hex stickers for your teammates.\n\n\n\nFigure 3: Sharing the swag!\n\n\n\n2 business days later\nThese were our favorite talks:\nHello Quarto: Share â€¢ Collaborate â€¢ Teach â€¢ Reimagine by Mine Cetinkaya-Rundel & Julia Stewart Lowndes gave us a glimpse of all the awesome things we could do with Quarto. We were super excited after watching this. And it seems like the community is keen too, because there is already an Awesome List for Quarto!\nHow Anchorage Built Alaskaâ€™s Vaccine Finder with R by Ben Matheson detailed the amazing work that the Anchorage Innovation Team did to develop and deploy a mobile vaccine finder. During the talk, Ben highlighted the importance of building and rolling out a MVP (minimum viable product).\nIntroducing workboots: Generate prediction intervals from tidymodel workflows by Mark Rieke introduced us to an R package for generating bootstrap prediction intervals within a tidymodel workflow. For a field like healthcare where uncertainty is relevant, this will be useful for us to incorporate in our projects, most of which are built with the tidymodels workflow.\nWhat they forgot to teach you about industry transitions from academia (WTF AITA) by Travis Gerke offered some great advice on poeple searching for jobs. While this talk focused on those transitioning from academia to industry, the materials presented will be useful to many! Check out the cool website!\nA Journey to Data Science: Tools for Equity and Diversity in STEM by Ileana Fenwick highlighted open science tools and communities to further equity and diversity in STEM.\nBuilding a ggplot2 rollercoaster: Creating amazing 3D data visualizations in R by Tyler Morgan-Wall showcased some cool things that can be done with ggplot2. Check out this awesome video!!\n2 months laterâ€¦?\nWe hope to achieve the following:\nCreate our own hex sticker!\nIntegrate Quarto into our workflows. We hope to move this blog to Quarto. This tutorial will be a great starting point.\nTry Python for Shiny. Our team has a mix of R and Python developers, some who have used Shiny and some who have not.\nCreate templates with consistent color schemes. This came up during many of the Quarto-related talks. A quick and easy way to make reporting easier is to use consistent branding.\nGive agency to newcomers during onboarding. This came up during many talks. The idea is to let newcomers contribute to the code on Day 1.\nShare learning materials. From Jeff Leekâ€™s closing keynote: â€œMentorship is a debt you donâ€™t pay off, you pay it forwardâ€. We want to share some of the cool work we do and resources/tutorials, what itâ€™s like working as a data scientist in healthcare. This blog is a step towards that.\n\nFOMO = fear of missing outâ†©ï¸Ž\nSome of our favorites: â€œObject of type â€˜closureâ€™ is not subsettableâ€ by Jenny Bryan, â€œOpen Source Software for Data Scienceâ€ by JJ Allaire, â€œDeploying End-To-End Data Science with Shiny, Plumber, and Pinsâ€ by Alex Gold.â†©ï¸Ž\nThe available pins were: â€œHugs okayâ€, â€œHandshakes or fist bumpsâ€, â€œElbowsâ€, and â€œPlease keep your distanceâ€â†©ï¸Ž\n",
    "preview": "posts/2022-08-04-posits-conf-time/rstudio_wall.jpg",
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-21-is-my-ai-discriminatory/",
    "title": "Is my AI Discriminatory?",
    "description": "A discussion about bias in healthcare AI, and building models with fairness and ethics in mind. [5 min read]",
    "author": [
      {
        "name": "Meggie Debnath",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [
      "post-journal-club",
      "bias-fairness-ethics"
    ],
    "contents": "\n\nContents\nBias is everywhere\nHealthcare data, like any data, is flawed\nBuilding fairer models\nTakeaways\nAdditional resources\n\nArtificial intelligence (AI) and machine learning (ML), in particular, are part of decision making across industries, the public sectors, and everywhere in between. From identifying fraudulent bank transactions to listing the shows and movies weâ€™re most likely to enjoy, AI is deeply embedded within our everyday lives. Oftentimes the results or outputs of these decisions are relatively harmless. However, increasingly, machine learning models are trained on complex and sensitive data, and used as part of decision making processes for diagnosing diseases or making hiring decisions. While these models have the ability to transform and improve lives, sometimes the decisions made or informed by AI can have far-reaching consequences.\nAs a part of our teamâ€™s bi-weekly journal clubs, we talked about sources of bias for AI models, the potential consequences and harms they can create, and what we can do as data scientists within the healthcare space.\nBias is everywhere\nBias is a part of human nature, coming from the limited view of the world that any single person or group can achieve. Whether implicitly or explicitly, this bias gets captured within our institutions and by extension - the data that we record. It can be reflected and amplified by artificial intelligence models that are trained using this data. Generally, the bias encoded within AI tools result in the greatest harm toward disadvantaged groups and people, such as racial minorities.\n\n\n\nFigure 1: Different types of bias that can exist when training machine learning models source\n\n\n\nThere are a few different ways bias can affect the prediction or decision made by an algorithm (Norori et al. 2021):\nHuman bias: underlying bias within the data caused by societal inequities and biased human decision-making\nData-driven bias: when training data is not representative of the population in which the algorithm will be applied\nAlgorithmic bias: when model development or training methods result in biased outcomes\nBy the same token, harms as a result of biased AI can manifest in different ways:\nHarm of allocation: if people are denied opportunities or resources based on the decision of an AI. An example of this is Amazonâ€™s resume screening tool that was biased against women because it was trained on data from the past 10 years, which was overwhelmingly male.\nHarm of quality-of-service: when an AI tool does not perform at the same level for one group as it does for another. An example of this may be voice assistants trained on predominantly male voices may have trouble recognizing the voices of women who use it.\nIn this way, AI can be a flawed reflection of our society and its systemic biases, and can become a â€œgatekeeperâ€ for jobs, medical treatments, and opportunities.\nHealthcare data, like any data, is flawed\nWithin the context of healthcare services, it is especially important to consider the types of bias within our data, as decisions made with the support of AI have the ability to influence critical decisions such as which patients receive additional care, or what medication dosages are prescribed. As with many other industries, healthcare and medical data can be biased, incorrect, missing, and incomplete.\nEven without the presence of AI tools, healthcare data holds implicit bias. For example, when visiting the emergency department for abdominal pain, men wait an average of 49 minutes before receiving an analgesic, whereas women wait an average of 65 minutes (Chen et al. 2008). The COVID-19 pandemic has also highlighted many existing racial inequities in healthcare, with the morbidity and mortality rate being higher for Black Americans, Native Americans, Pacific Islanders, and Hispanic/Latino patients compared with White Americans (Gawthrop 2022).\nWhen machine learning models are trained using data that already contains historical and societal inequities, these patterns are learned by the model, and the biases can be amplified when making predictions for new patients. Models that are deployed with underlying biases can disadvantage the groups who were under or mis-represented within the training data. For example, algorithms trained to identify disease within chest radiograph images were found to have higher underdiagnosis rates for female patients, patients under 20 years old, Black patients, and Hispanic patients. In other words, the risk of being falsely predicted as â€œhealthyâ€ were higher for these groups of people, meaning their clinical treatment would have been delayed or missed entirely (Seyyed-Kalantari et al. 2021).\nBuilding fairer models\nWe know that our models can contain harmful biases. But what can we do as data scientists in the healthcare space to ensure our models benefit the most people, and donâ€™t cause harm? This might be a daunting question, one that led to a lot more questions for our team:\nHow can we improve our development and monitoring processes to identify biases before they are deployed?\nShould we have a feedback loop to communicate data discrepancies and inform future data collection?\nHow can we better incorporate the patient experience and expertise?\nCan we incorporate recourse and contestability into our data science pipelines?\nBuilding fairer models is an iterative process, and one that requires more than one solution. Although not all are possible to implement everywhere, especially all at once, below are a few things our team is learning about and working on:\nUnderstanding sources and limitations of data. This involves thinking about where the data coming from and if thereâ€™s potential for any of the variables to be biased. For example, data containing a single variable â€œgenderâ€ with limited responses may actually be a mixture of sex or perceived gender, rather than reflecting a patientâ€™s true gender identity.\nBuilding models with an interdisciplinary and diverse team. When developing any kind of AI tool for clinical deployment, we heavily collaborate with the clinician teams that are involved. In addition, our project teams consist of people with varied backgrounds, experiences, cultures, and training.\nEvaluating model performance across sub-groups and applying techniques for improving explainability. There are many tools and resources for evaluating model fairness and understanding how a model performs for subgroups. Tools such as InterpretML and modelStudio.\nCreating and following standards for data, processes, models, and reporting. Standardization of these elements of a data science project ensures that there are clear guidelines and expectations, consistency among and across projects, and benchmarks to evaluate quality.\nMonitoring data, model usage, and performance over time. Monitoring how our models are performing after deployment is important to ensure there hasnâ€™t been any data drift or changes in the environment that may cause poor performance.\nLearning, discussing, and sharing. We believe itâ€™s important to keep learning, discussing through things like journal clubs, and where possible, sharing our processes, code, research, and learnings.\nAI has countless potential benefits, especially within healthcare - to improve patient care, hospital efficiency, and support decision-making. Working to build fairer models will help improve trust among clinically deployed AI tools, and ensure that all groups of people can benefit from the decisions made and supported by AI.\nTakeaways\nHuman bias, data-driven bias, and algorithmic bias are common ways in which a model might perform poorly for some patient subgroups, causing denial of opportunities or reducing quality of service\nThere is no perfect data, but there are different ways to combat bias and build models that are useful and reduce harms\nSome techniques for building better and fairer models include understanding sources of bias, building models considering multiple perspectives, evaluating them for fairness and explainability, and monitoring after deployment\nAdditional resources\nBelow are the full list of topics and readings that we dove into for our journal club series on bias, fairness, and ethics in healthcare AI.\nTopic\nReading Materials\nIntroduction to Bias, Fairness, and Ethics in AI\nMedicineâ€™s Machine Learning Problem\nIndigenous Data, Representing Race in AI, and Structural Racism in Healthcare\nStructural racism in precision medicine: leaving no one behind Racial Disparities and Mistrust in End-of-Life Care Dissecting racial bias in an algorithm used to manage the health of populations Racism and Health: Evidence and Needed Research The disturbing return of scientific racism\nMachine Learning Best Practices & Regulations\nFDA In Brief: FDA Collaborates with Health Canada and UKâ€™s MHRA to Foster Good Machine Learning Practice Algorithmic Impact Assessment tool Suicide hotline shares data with for-profit spinoff, raising ethical questions Their Bionic Eyes are now Obsolete and Unsupported\nFailure modes and Equity Concerns in Medical Imaging Models\nReading Race: AI Recognizes Patientâ€™s Racial Identity in Medical Images Under-diagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans\nBias and Assessing Model Fairness & Transparency\nMan is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings How to make sure your model is fair, accountable, and transparent AI FactSheets 360\nRepresenting Sex & Gender in AI and Healthcare Data\nTransgender-inclusive measures of sex/gender for population surveys: Mixed-methods evaluation and recommendations Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare\n\n\n\nChen, Esther H., Frances S. Shofer, Anthony J. Dean, Judd E. Hollander, William G. Baxt, Jennifer L. Robey, Keara L. Sease, and Angela M. Mills. 2008. â€œGender Disparity in Analgesic Treatment of Emergency Department Patients with Acute Abdominal Pain.â€ Academic Emergency Medicine 15 (5): 414â€“18. https://doi.org/10.1111/j.1553-2712.2008.00100.x.\n\n\nGawthrop, Elisabeth. 2022. â€œColor of Coronavirus: COVID-19 Deaths Analyzed by Race and Ethnicity.â€ APM Research Lab. https://www.apmresearchlab.org/covid/deaths-by-race.\n\n\nNorori, Natalia, Qiyang Hu, Florence Marcelle Aellen, Francesca Dalia Faraci, and Athina Tzovara. 2021. â€œAddressing Bias in Big Data and AI for Health Care: A Call for Open Science.â€ Patterns 2 (10): 100347. https://doi.org/10.1016/j.patter.2021.100347.\n\n\nSeyyed-Kalantari, Laleh, Haoran Zhang, Matthew B. A. McDermott, Irene Y. Chen, and Marzyeh Ghassemi. 2021. â€œUnderdiagnosis Bias of Artificial Intelligence Algorithms Applied to Chest Radiographs in Under-Served Patient Populations.â€ Nature Medicine 27 (12): 2176â€“82. https://doi.org/10.1038/s41591-021-01595-0.\n\n\n\n\n",
    "preview": "posts/2022-06-21-is-my-ai-discriminatory/ai-biases.jpg",
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-09-ooh-na-na-where-are-my-sodium-labs/",
    "title": "Ooh na na... where are my sodium labs?",
    "description": "The NA bug, or, what happens when the same word is used in different contexts. [5 min read]",
    "author": [
      {
        "name": "Chloe Pou-Prom",
        "url": {}
      }
    ],
    "date": "2022-05-09",
    "categories": [
      "language-R",
      "project-chartwatch",
      "post-miscellaneous"
    ],
    "contents": "\n\nContents\nSilent deployment\nMonitoring labs\nThe NA bug\nTakeaways\n\n\n\n\nSilent deployment\nOur team had been working actively on developing CHARTwatch, an early warning system for patients in general internal medicine at St.Â Michaelâ€™s Hospital. In November 2019 we were ready to move to a silent deployment phase, which means our entire pipeline was running (from data extraction to data processing to model prediction), but no outputs were going to the end-user.\nTypically, the goal of the silent deployment phase is to uncover unexpected behaviors with the data, system, or model. During model development and evaluation, we had only worked with historical extracts of the data. When moving from historical data to live data, thereâ€™s the risk of running into data issues (Cohen et al. 2021).\nThe data can be different due to external factors. For example, all of our models were trained on data prior to COVID-19, but shortly after the beginning of our silent deployment phase, we began to observe cases of COVID-19 in the hospital.\nThe data can be different due to data entry errors. For example, a body temperature could incorrectly be entered as 3700 Â°C instead of 37.00 Â°C.\nThe data can be different due to selection bias. For example, during training we excluded patients with really short and really long visits, as they were rare. However, we may encounter these kinds of visits in the live data.\nMonitoring labs\nWe had set up a monitoring dashboard to measure model inputs and model outputs. On close inspection, we made a discovery that was unquestionably oddâ€¦ no sodium labs had been measured since we had moved to silent testing!\n\n\n\n{\"x\":{\"attrs\":{\"title\":\"Electrolyte counts\",\"labels\":[\"day\",\"CA\",\"CL\",\"GLPOC\",\"K\",\"NA.\"],\"retainDateWindow\":false,\"axes\":{\"x\":{\"pixelsPerLabel\":60,\"drawAxis\":true},\"y\":{\"drawAxis\":true}},\"stackedGraph\":true,\"fillGraph\":false,\"fillAlpha\":0.15,\"stepPlot\":false,\"drawPoints\":false,\"pointSize\":1,\"drawGapEdgePoints\":false,\"connectSeparatedPoints\":false,\"strokeWidth\":1,\"strokeBorderColor\":\"white\",\"colorValue\":0.5,\"colorSaturation\":1,\"includeZero\":false,\"drawAxesAtZero\":false,\"logscale\":false,\"axisTickSize\":3,\"axisLineColor\":\"black\",\"axisLineWidth\":0.3,\"axisLabelColor\":\"black\",\"axisLabelFontSize\":14,\"axisLabelWidth\":60,\"drawGrid\":true,\"gridLineWidth\":0.3,\"rightGap\":5,\"digitsAfterDecimal\":2,\"labelsKMB\":false,\"labelsKMG2\":false,\"labelsUTC\":false,\"maxNumberWidth\":6,\"animatedZooms\":false,\"mobileDisableYTouch\":true,\"disableZoom\":false,\"legend\":\"auto\",\"labelsDivWidth\":400,\"labelsShowZeroValues\":true,\"labelsSeparateLines\":false,\"hideOverlayOnMouseOut\":true},\"scale\":\"daily\",\"annotations\":[],\"shadings\":[],\"events\":[],\"format\":\"date\",\"data\":[[\"2019-11-19T00:00:00.000Z\",\"2019-11-20T00:00:00.000Z\",\"2019-11-21T00:00:00.000Z\",\"2019-11-22T00:00:00.000Z\",\"2019-11-23T00:00:00.000Z\",\"2019-11-24T00:00:00.000Z\",\"2019-11-25T00:00:00.000Z\",\"2019-11-26T00:00:00.000Z\",\"2019-11-27T00:00:00.000Z\",\"2019-11-28T00:00:00.000Z\",\"2019-11-29T00:00:00.000Z\",\"2019-11-30T00:00:00.000Z\",\"2019-12-01T00:00:00.000Z\",\"2019-12-02T00:00:00.000Z\",\"2019-12-03T00:00:00.000Z\",\"2019-12-04T00:00:00.000Z\",\"2019-12-05T00:00:00.000Z\",\"2019-12-06T00:00:00.000Z\",\"2019-12-07T00:00:00.000Z\",\"2019-12-08T00:00:00.000Z\",\"2019-12-09T00:00:00.000Z\",\"2019-12-10T00:00:00.000Z\",\"2019-12-11T00:00:00.000Z\",\"2019-12-12T00:00:00.000Z\",\"2019-12-13T00:00:00.000Z\",\"2019-12-14T00:00:00.000Z\",\"2019-12-15T00:00:00.000Z\",\"2019-12-16T00:00:00.000Z\",\"2019-12-17T00:00:00.000Z\",\"2019-12-18T00:00:00.000Z\",\"2019-12-19T00:00:00.000Z\"],[18,23,18,19,18,18,20,15,21,11,20,15,15,14,14,9,14,12,7,12,17,13,14,12,21,15,15,24,24,22,13],[51,53,70,64,58,51,62,62,58,57,61,53,60,72,57,62,58,44,38,40,60,48,47,43,65,52,52,56,51,61,66],[56,41,50,56,54,66,51,56,52,60,72,52,79,74,83,91,77,64,60,61,57,68,59,62,55,42,59,65,63,50,70],[51,52,70,63,58,52,62,61,59,57,60,51,60,72,55,62,57,43,37,40,60,47,47,42,64,51,52,54,53,61,65],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],\"fixedtz\":false,\"tzone\":\"UTC\"},\"evals\":[],\"jsHooks\":[]}\nFigure 1: Daily counts of lab measurements: this includes counts for calcium (CA), chloride (CL), glucose (GLPOC), potassium (K), and sodium (NA).\n\n\n\nDid this make sense? NA! Sodium is measured in routinely ordered blood tests. Itâ€™ll usually get ordered alongside other tests (such as calcium, chloride, glucose, and potassium) as part of a basic metabolic panel. In Figure 1, we look at the daily counts of labs on units in which CHARTwatch was silently deployed. The other labs were regularly measured, but our pipeline had not detected a single sodium lab. There was NA way sodium would be missing!\nThe NA bug\nAfter hours of detective work, we found the issue:\nIn R, the programming language we used to develop CHARTwatch, the symbol NA stands for â€œnot availableâ€ and is used to represent missing data.\nIn chemistry, Na is the symbol used to represent the chemical element of sodium.\n\n\n\n{\"x\":{\"attrs\":{\"title\":\"Electrolyte counts\",\"labels\":[\"day\",\"CA\",\"CL\",\"GLPOC\",\"K\",\"NA.\"],\"retainDateWindow\":false,\"axes\":{\"x\":{\"pixelsPerLabel\":60,\"drawAxis\":true},\"y\":{\"drawAxis\":true}},\"stackedGraph\":true,\"fillGraph\":false,\"fillAlpha\":0.15,\"stepPlot\":false,\"drawPoints\":false,\"pointSize\":1,\"drawGapEdgePoints\":false,\"connectSeparatedPoints\":false,\"strokeWidth\":1,\"strokeBorderColor\":\"white\",\"colorValue\":0.5,\"colorSaturation\":1,\"includeZero\":false,\"drawAxesAtZero\":false,\"logscale\":false,\"axisTickSize\":3,\"axisLineColor\":\"black\",\"axisLineWidth\":0.3,\"axisLabelColor\":\"black\",\"axisLabelFontSize\":14,\"axisLabelWidth\":60,\"drawGrid\":true,\"gridLineWidth\":0.3,\"rightGap\":5,\"digitsAfterDecimal\":2,\"labelsKMB\":false,\"labelsKMG2\":false,\"labelsUTC\":false,\"maxNumberWidth\":6,\"animatedZooms\":false,\"mobileDisableYTouch\":true,\"disableZoom\":false,\"legend\":\"auto\",\"labelsDivWidth\":400,\"labelsShowZeroValues\":true,\"labelsSeparateLines\":false,\"hideOverlayOnMouseOut\":true},\"scale\":\"daily\",\"annotations\":[],\"shadings\":[],\"events\":[],\"format\":\"date\",\"data\":[[\"2019-11-19T00:00:00.000Z\",\"2019-11-20T00:00:00.000Z\",\"2019-11-21T00:00:00.000Z\",\"2019-11-22T00:00:00.000Z\",\"2019-11-23T00:00:00.000Z\",\"2019-11-24T00:00:00.000Z\",\"2019-11-25T00:00:00.000Z\",\"2019-11-26T00:00:00.000Z\",\"2019-11-27T00:00:00.000Z\",\"2019-11-28T00:00:00.000Z\",\"2019-11-29T00:00:00.000Z\",\"2019-11-30T00:00:00.000Z\",\"2019-12-01T00:00:00.000Z\",\"2019-12-02T00:00:00.000Z\",\"2019-12-03T00:00:00.000Z\",\"2019-12-04T00:00:00.000Z\",\"2019-12-05T00:00:00.000Z\",\"2019-12-06T00:00:00.000Z\",\"2019-12-07T00:00:00.000Z\",\"2019-12-08T00:00:00.000Z\",\"2019-12-09T00:00:00.000Z\",\"2019-12-10T00:00:00.000Z\",\"2019-12-11T00:00:00.000Z\",\"2019-12-12T00:00:00.000Z\",\"2019-12-13T00:00:00.000Z\",\"2019-12-14T00:00:00.000Z\",\"2019-12-15T00:00:00.000Z\",\"2019-12-16T00:00:00.000Z\",\"2019-12-17T00:00:00.000Z\",\"2019-12-18T00:00:00.000Z\",\"2019-12-19T00:00:00.000Z\",\"2019-12-20T00:00:00.000Z\",\"2019-12-21T00:00:00.000Z\",\"2019-12-22T00:00:00.000Z\",\"2019-12-23T00:00:00.000Z\",\"2019-12-24T00:00:00.000Z\",\"2019-12-25T00:00:00.000Z\",\"2019-12-26T00:00:00.000Z\",\"2019-12-27T00:00:00.000Z\",\"2019-12-28T00:00:00.000Z\",\"2019-12-29T00:00:00.000Z\",\"2019-12-30T00:00:00.000Z\",\"2019-12-31T00:00:00.000Z\",\"2020-01-01T00:00:00.000Z\",\"2020-01-02T00:00:00.000Z\",\"2020-01-03T00:00:00.000Z\",\"2020-01-04T00:00:00.000Z\",\"2020-01-05T00:00:00.000Z\",\"2020-01-06T00:00:00.000Z\",\"2020-01-07T00:00:00.000Z\",\"2020-01-08T00:00:00.000Z\",\"2020-01-09T00:00:00.000Z\",\"2020-01-10T00:00:00.000Z\",\"2020-01-11T00:00:00.000Z\",\"2020-01-12T00:00:00.000Z\",\"2020-01-13T00:00:00.000Z\",\"2020-01-14T00:00:00.000Z\",\"2020-01-15T00:00:00.000Z\",\"2020-01-16T00:00:00.000Z\",\"2020-01-17T00:00:00.000Z\",\"2020-01-18T00:00:00.000Z\",\"2020-01-19T00:00:00.000Z\",\"2020-01-20T00:00:00.000Z\"],[18,23,18,19,18,18,20,15,21,11,20,15,15,14,14,9,14,12,7,12,17,13,14,12,21,15,15,24,24,22,13,24,23,17,30,32,25,20,29,16,20,20,16,18,23,21,23,26,21,16,17,14,15,14,16,16,21,22,19,24,25,23,33],[51,53,70,64,58,51,62,62,58,57,61,53,60,72,57,62,58,44,38,40,60,48,47,43,65,52,52,56,51,61,66,72,66,62,87,77,60,81,79,70,67,61,53,53,82,76,74,85,76,91,85,75,74,73,63,72,65,80,65,68,62,54,74],[56,41,50,56,54,66,51,56,52,60,72,52,79,74,83,91,77,64,60,61,57,68,59,62,55,42,59,65,63,50,70,68,69,82,90,82,80,84,70,87,98,67,62,64,113,100,87,105,75,91,109,120,109,92,82,91,65,79,69,58,48,58,62],[51,52,70,63,58,52,62,61,59,57,60,51,60,72,55,62,57,43,37,40,60,47,47,42,64,51,52,54,53,61,65,72,66,62,86,77,60,81,79,70,66,61,53,53,79,75,74,86,76,90,84,75,74,72,63,72,64,80,66,67,62,55,73],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,68,66,62,87,77,60,81,79,70,67,61,53,53,82,76,75,85,77,91,84,75,74,73,63,72,65,80,65,68,62,54,73]],\"fixedtz\":false,\"tzone\":\"UTC\"},\"evals\":[],\"jsHooks\":[]}\nFigure 2: Daily counts of lab measurements after fixing the NA bug\n\n\n\nDepending on the context, the symbol meant something different! Our data extraction pipeline was interpreting the chemical element Na as â€œnot available!â€\nThe fix was quite straightforward. We updated the parameters of one of our function calls to specify that \"\" (empty string) should be used to represent â€œnot available,â€ instead of \"NA\". From the documentation of the RODBC package:\n\nna.strings: character string(s) to be mapped to NA when reading character data, default â€œNAâ€\n\nAfter deploying this fix, sodium counts were back to normal (as seen in Figure 2).\nWhile the fix was a simple one-line change, the problem we uncovered lead to plenty of follow-up questions!\nWere there other cases where the same symbol meant two different things based on the context?\nWhat does our electronic health record use to represent a missing value? Do they go with a number thatâ€™s biologically impossible? (e.g., a body temperature of -1000) Do they use a specific symbol/term? (e.g., â€œnot measured,â€ â€œmissingâ€)\nHow are these decisions made?\nRecently, thereâ€™s been a push for improvement in data quality standards, such as â€œDatasheets for Datasetsâ€ (Gebru et al. 2021) and the explosion of features stores, model stores, and evaluation stores1.\nTakeaways\nNA (sodium) â‰  NA (not available)\nSilent deployment is important.\nThorough metadata and data quality standards are important to mitigating these kinds of issues.\n\n\n\nCohen, Joseph Paul, Tianshi Cao, Joseph D. Viviano, Chin-Wei Huang, Michael Fralick, Marzyeh Ghassemi, Muhammad Mamdani, Russell Greiner, and Yoshua Bengio. 2021. â€œProblems in the Deployment of Machine-Learned Models in Health Care.â€ CMAJ 193 (35): E1391â€“94. https://doi.org/10.1503/cmaj.202066.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal DaumÃ© III, and Kate Crawford. 2021. â€œDatasheets for Datasets.â€ Communications of the ACM 64 (12): 86â€“92. http://arxiv.org/abs/1803.09010.\n\n\nWhat kind of â€œstoreâ€ do we think is next? ðŸ¤”â†©ï¸Ž\n",
    "preview": "posts/2022-05-09-ooh-na-na-where-are-my-sodium-labs/preview.PNG",
    "last_modified": "2023-12-20T16:06:02-05:00",
    "input_file": {}
  }
]
