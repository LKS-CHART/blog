---
title: "Reproducible medical imaging software environments, or, living in the future is hard."
description: |
  Building a reproducible environment for building medical imaging machine learning models
author:
  - name: Chris Hammill
date: 2023-01-27
output:
  distill::distill_article:
    self_contained: false
categories:
  - post-miscellaneous
draft: true
#preview: this_is_fine.png
---

I want to tell a story, advertise a product, and gripe about technology all at the same time. For readers
who don't know me, I'm Chris Hammill, a senior data scientist on Unity Health Toronto's Data Science and Advanced
Analytics team. I've been thinking for a long time about how to build software environments that are resilient to the
ravages of time and that others can pick up and use effectively at their leisure. So when in the course of my work
at DSAA I needed to develop an effective environment for doing machine learning on medical imaging data, I jumped at
the chance. The fruit of that labour is:

https://github.com/LKS-CHART/medical-imaging-nix

A [nix](https://nixos.org/) flake based environment that can be used as a starting point for future projects. The environment contains
R, python, jupyter, a suite of tools for working with medical imaging files, pytorch and tensorflow with
GPU support enabled. All^[well, most anyway] the things a medical imaging data scientist needs to hit the ground 
running on a new project. The environment can be compiled into a singularity container^[I will probably switch to or add 
docker before long] for portability to places that don't have nix. To get started with the environment you need
nix installed with flakes enabled but a new project can be initialized as simply as:

```sh
mkdir my-awesome-project
cd my-awesome-project
nix flake init -t github:LKS-CHART/medical-imaging-nix
nix develop
```

the first time you run this it will take a very long time, because it will build most of the software universe from
scratch for you^[including pytorch and tensorflow, let it run overnight.]. But after that all subsequent calls
to nix develop will drop you into your shiny new environment. If you're ready to package up your environment into
a container you can run

```sh
nix build
```

this also takes a while, so start it before a meeting. But once that's done you can send your environment
image to anywhere it might be needed.

## How we got here

One promising technology for reproducible and declarative software environments is the [nix package manager](https://nixos.org/)^[There
is also an operating system based on the package manager but we'll ignore that].

> you, a savvy expert: but Chris, what about conda, renv, groundhog, pacman, rstudio package manager, and docker?  

> me, an esoteric technology astronaut: what about 'em?

before I talk about why someone might consider going the nix route it might be worth talking a bit about what nix is.

## What is nix

Nix^[Nix can mean a lot of things: https://www.haskellforall.com/2022/08/stop-calling-everything-nix.html] is a package manager,
a build system, and a programming language. The programming language allows you to write code that builds software, the build
system builds the software and caches it in a content addressable store, and the package manager gives you access to those packages.

This allows you to generate supercharged versions of conda environments, renvs, and can even obviate the need to use docker. I'll
explain the advantages in a moment.

The nix ecosystem also provides a curated set of a packages referred to as nixpkgs, akin to debian packages available from apt, or conda.
Nixpkgs is the largest set of packages provided by any package manager^[https://repology.org/, but this is a bit of a cheat, the total number
includes, for example, all of the packages from CRAN], so on paper this should mean that building environments with nix should be easier than any of the alternatives, you can just grab your packages from the massive set of
packages in nixpkgs.

## But what's different?

Building software is really hard. If you've ever wanted to build someone's project from source, especially complicated modern projects, it can be
a trying ordeal. Software can be built with make, cmake, ymake, autotools, setuptools, R CMD INSTALL, ninja, bazel, shake, and many
many more build systems and tools.

To deal with this massive complexity, package managers typically try to shield you from all of this by giving you binary artifacts that are compatible with your "architecture". While the nix ecosystem provides cached binary builds of recent versions of the package set, it is by nature a source code
first package manager. The extremely brilliant devs who built out nix build system found ways to hook into most of the other common build systems, and
then package contributors used those tools to create reproducible recipes to build each of the 81k+ packages in nix. This means that
once you've pinned a version of your software, as long as that source code remains available you can rebuild your project from scratch.

By contrast conda has packaged 8000 packages, ubuntu offers an admirable 36k, these are built for you, which is convenient from a startup time
perspective, but you can't tweak the build instructions to, for example, support newer CPU instructions, or depend on a slightly newer version
of a dependency. Further, most pre-packaged builders assume something about the directory layout of your system, and they
can be broken by updating system packages using other package manager.

Package managers and build systems not playing nice together may sound like puritanical nerd worries, but these are real issues
I've experienced in practice. I was soured on conda long ago when even having it on my executable path broke my ability to build the R package
I was developing for work. What happened? Conda's addition to my `PATH` overrode my system `h5cc` a compiler wrapper for building C libraries
that depend on HDF5, this linked in the wrong version of HDF5 and prevented my R package from building. I had not asked conda for h5cc, it was
pulled in as a dependency of some arbitrary conda package I was using. It's wasn't just conda's fault, the R package itself was an eldritch horror 
of an autotools build, ditching conda was easier than fixing the R build to ignore conda's h5cc.

Still don't believe me that this is a real issue? What about [silently getting the wrong results](https://community.rstudio.com/t/rstudio-server-returns-wrong-results-without-any-message/54537/2) for your numerical code because
you had `LD_LIBRARY_PATH` set incorrectly, oof. Dynamically linking C programs/libraries relies on a byzantine collection of different types of 
mutable global state. There is your system default library locations (global state, disk-backed), `LIBRARY_PATH`/`LD_LIBRARY_PATH` (global state, 
shell environment local and disk-backed), pkg-config is sometimes used to configure linking (disk backed global-state pointing 
to disk-backed global state). Global state has the potential to do like state and change... state. This means your results are balanced
on a house of cards of mutable global state.

Nix gets around this by not using the system default libraries wherever possible, avoiding `LIBRARY_PATH`/`LD_LIBRARY_PATH` wherever possible^[it
does this by patching the produced compiled artifacts to point to their "dynamic" dependencies statically using \@rpath], and making the places
where state is unavoidable immutable (your nix store of built things is read only). So builds are hermetic and isolated, you can happily have 
multiple versions of the same C library running around without paying any extra attention to where your C dependencies are coming from. 
This means you don't need a separate docker container to have an alternate universe of C libraries to make sure your analysis works, you just 
have it beside all your normal stuff, and that's relevatory when you've been bitten by these problems enough times.

Nix also empowers you to be your own package repository, significant effort has gone into making builds fully deterministic where possible.
This means once I've built "pySweetDataToolR.jl" I can give it to you, if we're on the same architecture you can just slot the relevant parts
of my nix store into yours, so for a medium or larger organization you can set up a global cache of nix builds on a server that can be downloaded
by each user. For smaller orgs you might be able to get away with a single nix-store that everyone can share. No more N numpys per employee.

## Where's the rub?

So far I might have sounded effusive, if not fanboyish about the space alien wizard technology that can replace apt, conda, renv, etc. but
there are real and significant sharp corners to nix and nixpkgs especially for data science. First off while nixpkgs includes every single
package available on CRAN at the time of last snapshot, its coverage of pypi is piddly. Nixpkgs includes ~5200 python3.10 packages, whereas
pypi has ~432k packages. In order to put together the data science environment I built, I needed to package or modify 33 python packages. Some 
medical imaging related, some for working with jupyter notebooks, some machine learning related. And while generally not very 
challenging once I got the hang of it, some are quite thorny to package. Most are 
properly built from source, but some are just a thin wrapper around the wheels available on pypi, which defeats the purpose of nix^[although
I would argue incrementally better is still better, a few risky packages is better than all risky packages.]. Nix also encourages you to
run the full test suite for the packages, but often test code is scrubbed from packages on pypi, so unless you get the code from github you
may not have the tests. And you might need to disable some tests because nix's test environment is immutable which sometimes breaks test code,
so either you have to patch the tests yourself or you turn them off (so guess which you choose if you're pushing for a deadline).

So life in the future is tough, because you become part of the team building it. The future I mean. Since starting my nix journey several
years ago I've contributed code to nixpkgs more than a few times, but when you are under pressure to achieve actual business goals it can
be very frustrating to have to solve these problems yourself. 

The other place I've found nixpkgs to be frustrating to use for python, is upgrading. Unless you're maintaining your own branch of nixpkgs
you are somewhat at the whims of other nixpkg contributors as to what gets upgraded when. Often upgrading one package will break many packages
that depend on it, not only because the code becomes incompatible, but because it is fashionable in python packaging to set strict version
upper bounds, so the package won't even build (so we can't check if all the tests still pass with the new version). So I find myself in the position
of checking out old versions of files from nixpkgs to build my package overlays when I need to downdate a certain dependency. This is
tedious and I should probably switch to maintaining my own version of nixpkgs with my downdates and modifications, but this makes it harder
to share with others.

So this points to the biggest advantage of conda over nix, when conda does not have a package it gets it from pypi, nixpkgs cannot fail over
to grabbing from pypi and installing with pip, it also can't do dependency solving, if the nixpkgs version of a python lib isn't compatible with
another, you have to go find a satisfactory version yourself (or cheat and [lie about the version requirements](https://github.com/LKS-CHART/medical-imaging-nix/blob/2e811b3eb9b931f708d5f4b3658506b496ce24a0/container/overlay.nix#L71)). This is painful. There are two nix projects that have aimed to address this
problem, [mach-nix](https://github.com/DavHau/mach-nix) which has been abandoned^[although hopefully returning as part of https://github.com/nix-community/dream2nix in the future], and [poetry2nix](https://github.com/nix-community/poetry2nix) which may solve
some of my woes but I haven't tried it yet.

Another advantage alluded to already is speed. Compiling things takes time, so unless you're always getting prebuilt binaries from the
nix servers you can be in for long build times. Nix is good at not duplicating build work, but sometimes rebuilding is unavoidable. Say
you want to use a newer version of cuda, or gcc, most of your environment will need to be rebuilt. With other package managers you get
the cuda they give you, so there's no rebuilding to do.

Finally, the last difficulty is with irreducible system dependencies. Nixpkgs, when you're not using the nix operating system, does
not interact well with graphics drivers, there is a wrapper project I use called [NixGL](https://github.com/guibou/nixGL) which gives 
you access to graphics drivers and allows you to run programs that use CUDA^[you need to start programs with `nixGL<driver> <program>` which
is irritating, but I think can be fixed in my flake]. However this has meant I needed to hardcode my graphics
driver into the nix flake, severely hindering portability^[Ok you'd need to edit one line in the flake but that's enough of a barrier
to discourage users. If anyone in the nix community can help me solve this problem I'd be extremely grateful]. 

## Is this worth it?

I don't know. I've sunk considerable cost into building this environment so one might reasonably expect me to be quite biased. 

Would cutting down my effort by 75% at the risk of some amount of computation irreproducibility and unportability be worth it?

Is this a case of good enough practices trumping best practices? 

I'm not sure, but now that it's built I know that I will be able to revisit this exact environment for many years to come. I've acquired the
skills to fix package sets as issues arise, so it's not a large burden any longer to develop. If I can iron out a few more details about
merging python package sets from multiple versions of nixpkgs I would think that this is substantively superior to managing my software
with conda, apt, renv, and docker.

Would I encourage others to adopt the strategy of creating a nix environment from scratch for each new project? Probably not, at least don't
go it alone. Sharing working flakes and overlays for data scientists to springboard off of makes this type of reproducible, portable, futuristic
software versioning possible, but keep your [novelty budget](https://shimweasel.com/2018/08/25/novelty-budgets) in mind, you might be wise to
[pick boring technology](https://boringtechnology.club/). 


